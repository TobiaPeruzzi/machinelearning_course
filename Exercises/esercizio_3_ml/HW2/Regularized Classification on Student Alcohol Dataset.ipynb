{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Classification on Student Alcohol Dataset\n",
    "\n",
    "We are going to use a dataset from Kaggle (https://www.kaggle.com/uciml/student-alcohol-consumption)\n",
    " \n",
    "### Dataset description\n",
    "\n",
    ">The data were obtained in a survey of students from the portuguese language courses in a secondary school. It contains a lot of interesting social, gender and study information about students.\n",
    "\n",
    "> Have a look at the information about the dataset at the webpage: https://www.kaggle.com/uciml/student-alcohol-consumption\n",
    "\n",
    ">In this context, we ask you to estimate which students are the most prone to alcohol consumption given some social and educational information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: put your Surname, Name and ID number (\"numero di matricola\")\n",
    "\n",
    "\n",
    "Student Name: Tobia Peruzzi\n",
    "\n",
    "ID Number: 1185463\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The seed will be used as seed for splitting the data into training and test. \n",
    "# You can place your ID or also try different seeds to see the impact of the random subdvision of the train and test sets\n",
    "# and of the random components in the algorithm on the results\n",
    "IDnumber = 1185463\n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load library for plotting\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Load the data from a .csv file. In this notebook we use the pandas (Python Data Analysis Library) package, since it provides useful functions to clean the data. In particular, it allows us to remove samples with missing data, as we do below. We also plot some descriptions of columns, check the pandas documentation for 'describe()' if you want to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drink_alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>16.744222</td>\n",
       "      <td>2.514638</td>\n",
       "      <td>2.306626</td>\n",
       "      <td>1.568567</td>\n",
       "      <td>1.930663</td>\n",
       "      <td>3.930663</td>\n",
       "      <td>3.180277</td>\n",
       "      <td>3.184900</td>\n",
       "      <td>3.536210</td>\n",
       "      <td>3.659476</td>\n",
       "      <td>11.906009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498314</td>\n",
       "      <td>1.218138</td>\n",
       "      <td>1.134552</td>\n",
       "      <td>1.099931</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>1.051093</td>\n",
       "      <td>1.175766</td>\n",
       "      <td>1.446259</td>\n",
       "      <td>4.640759</td>\n",
       "      <td>3.230656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       drink_alcohol         age        Medu        Fedu  traveltime  \\\n",
       "count     649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean        0.454545   16.744222    2.514638    2.306626    1.568567   \n",
       "std         0.498314    1.218138    1.134552    1.099931    0.748660   \n",
       "min         0.000000   15.000000    0.000000    0.000000    1.000000   \n",
       "25%         0.000000   16.000000    2.000000    1.000000    1.000000   \n",
       "50%         0.000000   17.000000    2.000000    2.000000    1.000000   \n",
       "75%         1.000000   18.000000    4.000000    3.000000    2.000000   \n",
       "max         1.000000   22.000000    4.000000    4.000000    4.000000   \n",
       "\n",
       "        studytime      famrel    freetime       goout      health    absences  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean     1.930663    3.930663    3.180277    3.184900    3.536210    3.659476   \n",
       "std      0.829510    0.955717    1.051093    1.175766    1.446259    4.640759   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "25%      1.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "50%      2.000000    4.000000    3.000000    3.000000    4.000000    2.000000   \n",
       "75%      2.000000    5.000000    4.000000    4.000000    5.000000    6.000000   \n",
       "max      4.000000    5.000000    5.000000    5.000000    5.000000   32.000000   \n",
       "\n",
       "            Marks  \n",
       "count  649.000000  \n",
       "mean    11.906009  \n",
       "std      3.230656  \n",
       "min      0.000000  \n",
       "25%     10.000000  \n",
       "50%     12.000000  \n",
       "75%     14.000000  \n",
       "max     19.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load pands and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# this time we use pandas to load and clean the dataset\n",
    "\n",
    "# read the data from the csv file\n",
    "df = pd.read_csv(\"data/student-data.csv\", sep=',')\n",
    "\n",
    "# let's see some statistics about the data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we create data matrices: many of the features are categorical, so we need to encode them with ***indicator matrices*** (i.e., using the so called one-hot encoding). That is, if a feature can take $\\ell$ different values $v_1,\\dots,v_{\\ell}$, we create $\\ell$ indicator (0-1) features $I_1,\\dots,I_{\\ell}$, such that $I_{j} = 1$ if and only if the value of the feature is $v_j$. This can be done in Python by first encode a feature with integers with LabelEncoder() and then obtain the indicator variables with OneHotEncoder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drink_alcohol', 'school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'guardian', 'traveltime', 'studytime', 'famrel', 'freetime', 'goout', 'health', 'absences', 'Marks']\n",
      "Number of samples: 649\n",
      "Categorical feature: school    Number of categories: (2,)\n",
      "Categorical feature: sex    Number of categories: (2,)\n",
      "Valued feature: age\n",
      "Categorical feature: address    Number of categories: (2,)\n",
      "Categorical feature: famsize    Number of categories: (2,)\n",
      "Categorical feature: Pstatus    Number of categories: (2,)\n",
      "Categorical feature: Medu    Number of categories: (5,)\n",
      "Categorical feature: Fedu    Number of categories: (5,)\n",
      "Categorical feature: Mjob    Number of categories: (5,)\n",
      "Categorical feature: Fjob    Number of categories: (5,)\n",
      "Categorical feature: guardian    Number of categories: (3,)\n",
      "Categorical feature: traveltime    Number of categories: (4,)\n",
      "Categorical feature: studytime    Number of categories: (4,)\n",
      "Categorical feature: famrel    Number of categories: (5,)\n",
      "Categorical feature: freetime    Number of categories: (5,)\n",
      "Categorical feature: goout    Number of categories: (5,)\n",
      "Categorical feature: health    Number of categories: (5,)\n",
      "Valued feature: absences\n",
      "Valued feature: Marks\n",
      "Shape of X: (649, 64)\n",
      "Sample element from X: [1.0 0.0 0.0 1.0 15 0.0 1.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0\n",
      " 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0\n",
      " 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0\n",
      " 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0 14]\n"
     ]
    }
   ],
   "source": [
    "#df.values contains the data, both the values of instances and the value of the label\n",
    "Data = df.values\n",
    "# the matrix including the categorical data is given by columns from the second one \n",
    "X_categorical = Data[:,1:]\n",
    "# the target value (class) is in the first column\n",
    "Y = Data[:,0]\n",
    "\n",
    "print(list(df))\n",
    "\n",
    "# get the number d of features of each sample\n",
    "d = X_categorical.shape[1]\n",
    "\n",
    "# get the number m of samples\n",
    "m = X_categorical.shape[0]\n",
    "\n",
    "#let's see what the number of samples is\n",
    "print(\"Number of samples: {}\".format(m))\n",
    "\n",
    "#now encode categorical variables using integers and one-hot-encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(categories='auto')\n",
    "\n",
    "# encode the first column of the data matrix into indicator variables\n",
    "\n",
    "X_tmp = label_encoder.fit_transform(X_categorical[:,0])\n",
    "X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "X = onehot_encoder.fit_transform(X_tmp[:,0].reshape(-1,1)).toarray()\n",
    "print(\"Categorical feature:\", df.columns[1], \"   Number of categories:\", X[1,:].shape)\n",
    "\n",
    "# repeat for the other categorical input variables\n",
    "index_categorical = [1,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "\n",
    "for i in range(1,19):\n",
    "    if i in index_categorical:\n",
    "        X_tmp = label_encoder.fit_transform(X_categorical[:,i])\n",
    "        X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "        X_tmp = onehot_encoder.fit_transform(X_tmp[:,0].reshape(-1,1)).toarray()\n",
    "        X = np.hstack((X,X_tmp))\n",
    "        print(\"Categorical feature:\", df.columns[i+1], \"   Number of categories:\", X_tmp[1,:].shape)\n",
    "    else:\n",
    "        X_tmp = X_categorical[:,i]\n",
    "        X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "        X = np.hstack((X,X_tmp))\n",
    "        print(\"Valued feature:\", df.columns[i+1])\n",
    "        \n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Sample element from X:\", X[20,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The class labels are already 0-1, so we can use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# properly encode the target labels\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "K = max(Y) + 1 # number of classes\n",
    "\n",
    "print(\"Number of classes: \"+str(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $m$ total data points, keep $m\\_training = 100$ data points as data for ***training and validation*** and $m\\_test = m - m\\_training$ as test data. Splitting is random, using as seed your ID number. Make sure that the training set contains at least 10 instances from each class.If it does not, modify the code so to apply a random\n",
    "permutation (or the same permutation multiple times) to the samples until this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1\n",
      " 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0]\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation data\n",
    "\n",
    "# load a package which is useful for the training-test splitting\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# number of samples\n",
    "m = np.shape(X)[0]\n",
    "\n",
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from class 1 and at least 10 elements\n",
    "#from class -1! If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "\n",
    "permutation = np.random.permutation(m)\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "\n",
    "m_training = 100 #  # use 100 samples for training + validation...\n",
    "m_test = m-m_training # and the rest for testing\n",
    "\n",
    "# test_size is the proportion of samples in the test set\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size =float(m_test)/float(m), random_state = IDnumber)\n",
    "\n",
    "print(Y_training)\n",
    "\n",
    "m_training = X_training.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "#let's see what the fraction of ones in the entire dataset is\n",
    "print(float(sum(Y_training)+sum(Y_test))/float(m_training+m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data to have zero-mean and unit variance (columnwise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Features Matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = X.astype(np.float64) #standard scaler works with double precision data\n",
    "X_training = X_training.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "#let's use the standard scaling; we degine the scaling for the entire dataset\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "#let's apply the scaling to the training set\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "#let's apply the scaling to the test set\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Logistic Regression\n",
    "\n",
    "We now perform logistic regression using the function provided by Scikit-learn.\n",
    "\n",
    "Note: as provided by Scikit-learn, logistic regression is always implemented using regularization. However, the impact of regularization can be dampened to have almost no regularization by changing the parameter $C$, which is the inverse of $\\lambda$. Therefore to have no regularization, which is $\\lambda = 0$ for the model seen in class, we need $C$ to have a large value. Here we fix $C = 100000000$.\n",
    "\n",
    "[Note that the intercept is estimated in the model.]\n",
    "\n",
    "For all our models we are going to use 10-fold cross validation to estimate the parameters (when needed) and/or estimate the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [100000000]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.64121212]\n",
      "Best value of parameter C according to 10-fold Cross-Validation: 100000000\n",
      "<class 'dict'>\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: [0.77777778]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# define a logistic regression model with very high C parameter -> low impact from regularization;\n",
    "# there are many solvers available to obtain the solution to the logistic regression problem, we just pick\n",
    "# one of them; 'cv' is the number of folds in cross-validation; we also specify l2 as regularization penalty,\n",
    "# just to pick one; Cs contains the values of C to be tested and to pick from with validation. Here we\n",
    "# are interested in only 1 value of C, and use cross-validation just to estimate the validation error\n",
    "# in a same way as other models\n",
    "\n",
    "reg = linear_model.LogisticRegressionCV(Cs=[100000000], solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "reg.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( reg.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_accuracies = np.divide(np.sum(reg.scores_[1],axis=0),10)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of parameter C according to 10-fold Cross-Validation: {}\".format( reg.C_[0] ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "print(type(reg.scores_))\n",
    "reg_best_CV_accuracy = max(reg.scores_[1])\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( reg_best_CV_accuracy ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the logistic regression function in Scikit-learn has many optional parameters. Read the documentation if you want to understand what they do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1\n",
    "### Learn the best model from Logistic Regression on the entire training set and examine coefficients (by printing and plotting them)\n",
    "\n",
    "Note that you can use simply $linear\\_model.LogisticRegression()$, that does not use cross-validation, without passing the best value of $C$ (and then fit())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set: [[  2.32605163  -2.32605163 -24.07533164  24.07533164  50.34094864\n",
      "    0.54655057  -0.54655057 -11.26623592  11.26623592   2.55083877\n",
      "   -2.55083877   1.60008704 -12.02792078  18.63408541   7.17455632\n",
      "  -14.73042113   1.47991074  -4.8097492   -9.28261988  16.78150821\n",
      "   -1.057245    -7.04112018   2.47257994   0.12464681  -6.11912803\n",
      "   14.77506564  21.31286711   2.85893548 -16.12905498  20.00576363\n",
      "  -29.48341428   8.3587814   -8.76451208   1.90834838  -0.17554021\n",
      "   -8.48860163  14.26236063   0.86276669   6.97184294  16.51923016\n",
      "   -7.59765371 -38.98361806   3.56759243   3.91001333   6.94009465\n",
      "   21.27100457 -32.61605017  11.20232033 -21.82372135  13.25744612\n",
      "   -2.86040417   0.23500865 -23.92008331 -33.84551626  20.69368324\n",
      "   24.40093275   1.80131612  19.31581078  10.86352143 -27.46898769\n",
      "  -14.44724284  12.28045187   4.34373179   5.02668531]]\n",
      "Intercept: [-16.60517564]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use a large C to disable regularization\n",
    "reg_full = linear_model.LogisticRegression(C=100000000, solver='newton-cg', penalty='l2')\n",
    "\n",
    "reg_full.fit(X_training, Y_training)\n",
    "\n",
    "# print the coefficients from the logistic regression model.\n",
    "print(\"Coefficients obtained using the entire training set: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( reg_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "reg_coef = reg_full.coef_.reshape(reg_full.coef_.shape[1],)\n",
    "plt.figure()\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.45       # the width of the bars\n",
    "plt.bar(ind, reg_coef, width, color='r')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5465505677640714]\n",
      "[0.5465505677640714, -0.5465505677640572]\n",
      "[0.5465505677640714, -0.5465505677640572, 0.1246468110909793]\n",
      "[0.5465505677640714, -0.5465505677640572, 0.1246468110909793, -0.17554020793164848]\n",
      "[0.5465505677640714, -0.5465505677640572, 0.1246468110909793, -0.17554020793164848, 0.862766687003775]\n",
      "[0.5465505677640714, -0.5465505677640572, 0.1246468110909793, -0.17554020793164848, 0.862766687003775, 0.2350086477932879]\n",
      "Number of coefficients between -1 and 1:  6\n"
     ]
    }
   ],
   "source": [
    "near_zero_coeff = []\n",
    "for i in range(len(reg_full.coef_[0])):\n",
    "    if reg_full.coef_[0,i] < 1 and reg_full.coef_[0,i] > -1:\n",
    "        near_zero_coeff.append(reg_full.coef_[0,i])\n",
    "        print(near_zero_coeff)\n",
    "print(\"Number of coefficients between -1 and 1: \", len(near_zero_coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "\n",
    "### Questions: How many coefficients do you get (recall that the one-hot encoding has been used for some of the features)? Why? How many of them are \"close\" to 0? What do negative coefficient values mean? (max 5 lines)\n",
    "\n",
    "### Answer to the questions\n",
    "I get 64 coefficients. That is also the number of categories after one-hot encoding has been used fro the features. Therefore I think that that the number of coefficients that I get is due to the encoding process.\n",
    "In the cell above is shown the number of coefficients \"close\" to 0 (included in a range between -1 and 1).\n",
    "I think that negative coefficient values mean that the odds ratio between the category and the reference category is less than 1, that is if the category increases, the reference category will decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 3\n",
    "### Predict labels on training and test\n",
    "\n",
    "- Compute the predicted labels on training and test data using reg.predict\n",
    " - Evaluate the accuracy using metrics.accuracy_score from scikit-learn (it returns the percentage of data correctly classified).\n",
    " - Evaluate the score used by logistic regression on training and test data using metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training dataset: 1.0\n",
      "Accuracy for test dataset: 0.6138433515482696\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# prediction on training data\n",
    "Y_training_prediction_LR = reg_full.predict(X_training)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "\n",
    "print(\"Accuracy for training dataset:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR))\n",
    "\n",
    "# prediction on test data\n",
    "Y_test_prediction_LR = reg_full.predict(X_test)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "\n",
    "print(\"Accuracy for test dataset:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "### Use L2 regularized logistic regression with cross-validation\n",
    "\n",
    "We perform the L2 regularization for different values of the regularization parameter $C$, and use the Scikit-learn function to perform cross-validation (CV).\n",
    "\n",
    "In L2 regularized logistic regression, the following L2 regularization term is added to the loss:\n",
    "\n",
    "$$\n",
    "    \\lambda \\sum_{i=1}^d w_i^2\n",
    "$$\n",
    "\n",
    "The parameter $C$ used by Scikit learn corresponds to the inverse of $\\lambda$, that is $C = \\frac{1}{\\lambda}$.\n",
    "\n",
    "Note: the CV in Scikit-learn is by default a *stratified* CV, that means that data is split into train-validation while maintaining the proportion of different classes in each fold.\n",
    "\n",
    "In the code below:\n",
    "- use LogisticRegressionCV() to select the best value of C with a 10-fold CV with L2 penalty;\n",
    "- use LogisticRegression() to learn the best model for the best C with L2 penalty on the entire training set\n",
    "\n",
    "Note that LogisticRegressionCV() picks some default values of C to try, but you may need to pass some other values in case for your dataset you need to explore a different interval of values. This applies every time that you use LogisticRegressionCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: 0.99\n",
      "Best value of C: [21.5443469]\n",
      "[[0.54545455 0.54545455 0.54545455 0.54545455 0.54545455 0.63636364\n",
      "  0.63636364 0.72727273 0.72727273 0.72727273]\n",
      " [0.54545455 0.54545455 0.63636364 0.72727273 0.72727273 0.63636364\n",
      "  0.54545455 0.54545455 0.54545455 0.54545455]\n",
      " [0.54545455 0.54545455 0.54545455 0.54545455 0.63636364 0.63636364\n",
      "  0.63636364 0.63636364 0.63636364 0.63636364]\n",
      " [0.54545455 0.54545455 0.72727273 0.54545455 0.45454545 0.45454545\n",
      "  0.63636364 0.63636364 0.63636364 0.63636364]\n",
      " [0.5        0.5        0.7        0.7        0.7        0.7\n",
      "  0.8        0.7        0.7        0.7       ]\n",
      " [0.5        0.5        0.5        0.6        0.6        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [0.55555556 0.55555556 0.66666667 0.66666667 0.55555556 0.55555556\n",
      "  0.55555556 0.66666667 0.66666667 0.66666667]\n",
      " [0.55555556 0.55555556 0.88888889 0.88888889 0.88888889 0.88888889\n",
      "  0.77777778 0.77777778 0.77777778 0.77777778]\n",
      " [0.55555556 0.55555556 0.77777778 0.66666667 0.66666667 0.66666667\n",
      "  0.77777778 0.66666667 0.66666667 0.66666667]\n",
      " [0.55555556 0.55555556 0.44444444 0.55555556 0.44444444 0.66666667\n",
      "  0.66666667 0.66666667 0.66666667 0.66666667]]\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.8888888888888888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([21.5443469]), class_weight=None, cv=10,\n",
       "                     dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "                     l1_ratios=None, max_iter=100, multi_class='warn',\n",
       "                     n_jobs=None, penalty='l2', random_state=None, refit=True,\n",
       "                     scoring=None, solver='newton-cg', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the model using LogisticRegressionCV passing an appropriate solver, cv value, and choice of penalty\n",
    "regL2 = linear_model.LogisticRegressionCV(solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "\n",
    "regL2.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( regL2.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_prediction = regL2.predict(X_training)\n",
    "\n",
    "CV_accuracies = metrics.accuracy_score(Y_training, CV_prediction)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of C: {}\".format( regL2.C_ ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "print(regL2.scores_[1])\n",
    "regL2_best_CV_accuracy = np.max(regL2.scores_[1])\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( regL2_best_CV_accuracy ))\n",
    "\n",
    "#define the model using the best C and an appropriate solver\n",
    "\n",
    "regL2_best = linear_model.LogisticRegressionCV(Cs=regL2.C_, solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model using the best C on the entire training set\n",
    "\n",
    "regL2_best.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 5: Print and plot the coefficients from logistic regression with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set without regularization: [[  2.32605163  -2.32605163 -24.07533164  24.07533164  50.34094864\n",
      "    0.54655057  -0.54655057 -11.26623592  11.26623592   2.55083877\n",
      "   -2.55083877   1.60008704 -12.02792078  18.63408541   7.17455632\n",
      "  -14.73042113   1.47991074  -4.8097492   -9.28261988  16.78150821\n",
      "   -1.057245    -7.04112018   2.47257994   0.12464681  -6.11912803\n",
      "   14.77506564  21.31286711   2.85893548 -16.12905498  20.00576363\n",
      "  -29.48341428   8.3587814   -8.76451208   1.90834838  -0.17554021\n",
      "   -8.48860163  14.26236063   0.86276669   6.97184294  16.51923016\n",
      "   -7.59765371 -38.98361806   3.56759243   3.91001333   6.94009465\n",
      "   21.27100457 -32.61605017  11.20232033 -21.82372135  13.25744612\n",
      "   -2.86040417   0.23500865 -23.92008331 -33.84551626  20.69368324\n",
      "   24.40093275   1.80131612  19.31581078  10.86352143 -27.46898769\n",
      "  -14.44724284  12.28045187   4.34373179   5.02668531]]\n",
      "Coefficients obtained using the entire training set with regularization: [[ 3.53115794e-01 -3.53115794e-01 -2.86940614e+00  2.86940614e+00\n",
      "   5.58665155e+00 -8.26702038e-03  8.26702038e-03 -1.15984789e+00\n",
      "   1.15984789e+00  2.42239002e-01 -2.42239002e-01  1.03172621e-04\n",
      "  -1.46328785e+00  2.00373000e+00  9.20561633e-01 -1.52599780e+00\n",
      "  -2.94087149e-05 -3.86245923e-01 -9.22696498e-01  1.65684884e+00\n",
      "  -1.57805150e-01 -2.86600836e-01  4.56581895e-02 -7.12473174e-02\n",
      "  -7.06678870e-01  1.35916679e+00  2.14565088e+00 -4.02736639e-04\n",
      "  -1.63921842e+00  2.18935485e+00 -3.04540199e+00  1.27261769e+00\n",
      "  -1.36599980e+00  3.50026622e-01  2.78061872e-01 -9.03577625e-01\n",
      "   1.13990602e+00 -1.83409831e-01  7.22762035e-01  1.39948310e+00\n",
      "  -2.92355979e-01 -4.13147436e+00  4.09053500e-01  2.27548128e-01\n",
      "   9.54982672e-01  2.50075029e+00 -3.83581854e+00  9.08938391e-01\n",
      "  -2.06994889e+00  1.43899903e+00 -4.30885255e-01  9.34550415e-02\n",
      "  -2.86676742e+00 -3.72976368e+00  1.86778140e+00  3.20542927e+00\n",
      "   3.03117410e-01  2.15610032e+00  1.28526340e+00 -3.22823662e+00\n",
      "  -1.02762916e+00  1.00514680e+00  5.44304864e-01  4.57227224e-01]]\n",
      "Intercept: [-2.10488743]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the coefficients from logistic regression\n",
    "\n",
    "print(\"Coefficients obtained using the entire training set without regularization: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "#print the coefficients from L2 regularized logistic regression\n",
    "\n",
    "print(\"Coefficients obtained using the entire training set with regularization: {}\".format( regL2_best.coef_ ))\n",
    "\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( regL2_best.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "regL2_best_coef = regL2_best.coef_.reshape(regL2_best.coef_.shape[1],)\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(ind, reg_coef, width, color='r')\n",
    "rects2 = ax.bar(ind + width, regL2_best_coef, width, color='g')\n",
    "ax.legend((rects1[0], rects2[0]), ('Log Regr', 'Log Regr + L2 Regul'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients: Standard and Regularized Version')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 6: how do the coefficients from logistic regression with L2 regularization compare to the ones from logistic regression without regularization ? Why? (max 5 lines)\n",
    "\n",
    "### Answer to the questions\n",
    "The coefficients obtainde from logistic regression with L2 regularization are all really close to zero opposed to the coefficients obtained from logistic regression without regularization. That is because if we take the best value for C, we are using the strongest regularization for this problem, and stronger regularizations are represented by smaller values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 7: obtain classification accuracy on training and test data for the L2 regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.99\n",
      "Test Accuracy: 0.6083788706739527\n",
      "Accuracy for training dataset: 1.0\n",
      "Accuracy for test dataset: 0.6138433515482696\n"
     ]
    }
   ],
   "source": [
    "#now get training and test error and print training and test accuracy\n",
    "\n",
    "# predictions on training data \n",
    "Y_training_prediction_LR_L2 = regL2_best.predict(X_training)\n",
    "\n",
    "# predictions on test data \n",
    "Y_test_prediction_LR_L2 = regL2_best.predict(X_test)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on training data\n",
    "print(\"Training Accuracy:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR_L2))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on test data\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR_L2))\n",
    "\n",
    "\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "\n",
    "print(\"Accuracy for training dataset:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "\n",
    "print(\"Accuracy for test dataset:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 8: How does training and test accuracies change when using regularization? Comment (max 5 lines)\n",
    "\n",
    "### Answer to the question\n",
    "Both the training and test accuracy obtained with L2 regularization are lower than the training and test accuracy obtained without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TO DO 9: Use larger datasets for training set\n",
    "\n",
    "Perform the same estimation procedures using different more points on the training data, that is fix $m_{training} = 400$. You can simply copy and paste all the code you have written previously into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 0 0 1\n",
      " 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0\n",
      " 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1\n",
      " 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1\n",
      " 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0]\n",
      "0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation data\n",
    "\n",
    "# number of samples\n",
    "m = np.shape(X)[0]\n",
    "\n",
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from class 1 and at least 10 elements\n",
    "#from class -1! If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "\n",
    "permutation = np.random.permutation(m)\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "\n",
    "m_training = 400 #  \n",
    "m_test = m-m_training # and the rest for testing\n",
    "\n",
    "# test_size is the proportion of samples in the test set\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size =float(m_test)/float(m), random_state = IDnumber)\n",
    "\n",
    "print(Y_training)\n",
    "\n",
    "m_training = X_training.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "#let's see what the fraction of ones in the entire dataset is\n",
    "print(float(sum(Y_training)+sum(Y_test))/float(m_training+m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Features Matrix\n",
    "\n",
    "X = X.astype(np.float64) #standard scaler works with double precision data\n",
    "X_training = X_training.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "#let's use the standard scaling; we degine the scaling for the entire dataset\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "#let's apply the scaling to the training set\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "#let's apply the scaling to the test set\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [100000000]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.66066291]\n",
      "Best value of parameter C according to 10-fold Cross-Validation: 100000000\n",
      "<class 'dict'>\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: [0.74358974]\n"
     ]
    }
   ],
   "source": [
    "# define a logistic regression model with very high C parameter -> low impact from regularization;\n",
    "# there are many solvers available to obtain the solution to the logistic regression problem, we just pick\n",
    "# one of them; 'cv' is the number of folds in cross-validation; we also specify l2 as regularization penalty,\n",
    "# just to pick one; Cs contains the values of C to be tested and to pick from with validation. Here we\n",
    "# are interested in only 1 value of C, and use cross-validation just to estimate the validation error\n",
    "# in a same way as other models\n",
    "\n",
    "reg = linear_model.LogisticRegressionCV(Cs=[100000000], solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "reg.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( reg.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_accuracies = np.divide(np.sum(reg.scores_[1],axis=0),10)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of parameter C according to 10-fold Cross-Validation: {}\".format( reg.C_[0] ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "print(type(reg.scores_))\n",
    "reg_best_CV_accuracy = max(reg.scores_[1])\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( reg_best_CV_accuracy ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set: [[ 0.03793378 -0.03793378 -0.27637114  0.27637114  0.23970661  0.02862305\n",
      "  -0.02862305 -0.1403771   0.1403771  -0.13457887  0.13457887 -0.01767398\n",
      "   0.17298298 -0.10103968  0.18737986 -0.22802273 -1.35823587  0.02460383\n",
      "   0.06148619  0.1000641   0.15205703 -0.08988196  0.12992073 -0.23434267\n",
      "   0.04439876  0.31553283  0.1000716  -0.29854566  0.06831823  0.0820583\n",
      "  -0.1751318   0.04964659 -0.06897475  0.04317071  0.02012928  0.04485624\n",
      "  -0.13782984  0.04528974  0.17407053 -0.17255942  0.10543026 -0.14655686\n",
      "   0.43395688  0.06704966  0.10916136  0.01369586 -0.31004021 -0.05157736\n",
      "   0.11002732 -0.01897788  0.03141853 -0.10611658 -0.39072114 -0.39682223\n",
      "   0.02843307  0.28456925  0.36509299 -0.07834111  0.04100621 -0.10458076\n",
      "   0.02771892  0.09157678  0.01020576 -0.40372469]]\n",
      "Intercept: [-0.41669887]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3debwcVZ338c+XhICyGJYrhiUkYFRwIWCDID5sgiKPQ0ABw4NMcMAMKoPLAII6M4jjCOqIOoISZckoCgyCRhHZl9ERyA1CIImBGJBkEknYBASBhN/zxzlXOk133751u293p7/v16tet+vU9qvqvv3rOqfqlCICMzOzoVqn3QGYmVl3cgIxM7NCnEDMzKwQJxAzMyvECcTMzApxAjEzs0KcQKzlJB0l6dqCy86TtE+TQ+p4kq6WNK3dcQyFpFdI+pmkP0n6r1z2r5IekfRHSeMlPS1p1CDr+T+SFo5M1DYc8n0gVk7Sg8BxEXF9G7Z9EbA0Ij43zPVMAB4A/pyLHgG+ExFnDme9awtJuwGnA28HXgQWAd+OiAuHud6jgX8A3h4RqyRtA9wHbBsRK4YXdeGYHqRNn+de4DMQW5uNjYgNgcOAf5J0QLM3IGl0s9fZSpL2AG4EbgFeC2wGfAR4TxNWvy1wX0SsKht/tF3Jw0ZARHjw8NcBeBDYv8a0D5N+rT4GzAK2LJv2LmAh8CfgXNIX1HF52jHAr/JrAWcDK/K8c4E3AdOBF4DngaeBn1XGA4wCPgP8HngKmANsUyXOCUAAo8vK7gBOLhvfEvgxsJJ0tnJi2bRXADOBx4EFwCmkM6PyY/TpHPtzwOhB1rcb0A88CTwMfC2Xrw/8AHgUeAKYDWyRp91cdvzWAT4H/CEft/8EXlWxr9OAh0hnW5+t8/7+CjhnkM9Avff5DcB1edpC4Ihc/vn83r2Q37+/B54lneE8DVxU+b4AmwIXAsvysf5JLt+n4njXO7anA5flY/IUMA8o5Wnfz9t/NsdwSr1j7qHA90W7A/DQWQM1EgiwX/5y2gVYD/gP4NY8bfP85fi+/GX68fxFUi2BvJv0xT+WlEx2AMblaRcB/1orHuBk4B7g9XnZnYDNqsRa+UW1O/AMcGgeXyfH8M/AGGA7YDHw7jz9TFIC3ATYmpQoKhPIXcA2pGQz2Pp+AxydX28I7J5f/z3wM+CVpOT4VmDjPO3msuP3d6Qv9O3y8lcA36/Y1+/mWHYiJbUdqhyXVwKrgX3rvP/13ucNgCXAh/L7vEue9415+unAD8rWtU/Fcat8X64CLs3HeV1g78rlGji2pwN/AQ7Kx/BLwG21Ps/1jrmHoQ+uwrJGHQVcEBF3RsRzwGnAHrm94SBgXkRcEan64pvAH2us5wVgI9IvWUXEgohY3mAMxwGfi4iFkdwdEY/Wmf8RSc+SvsDPBX6Sy3cF+iLijIh4PiIWk76Ap+bpRwD/FhGPR8TSvD+VvhkRSyLi2QbW9wLwWkmbR8TTEXFbWflmwGsjYnVEzImIJ6ts6yjSWcviiHiadOynVlSffT4ino2Iu4G7SYmk0iakL+R6x7ve+/xe4MGIuDAiVkXEnaQzg8PqrK8qSeNI1WbH5+P8QkTcUmXWwY4tpB8nv4iI1aSzjmr7PqDRY24NcAKxRm1JqkIBIH+RPQpslactKZsWwNJqK4mIG4FvAecAD0uaIWnjBmPYhlR91ajNSb/YTyL9ql03l28LbCnpiYGBVDW2RZ6+xv5UvK5WNtj6jgVeB/xO0mxJ783l3weuAS6RtEzSlyWty8utcezz69Fl64c1E/Yzeb8rPU6q0hlXZVrVbVW8z9sCb6vYz6OA19RZXy3bAI9FxOODzDfYsYWX7/v6ddqmGj3m1gAnEGvUMtI/MwCSNiD9kvtf0i/arcumqXy8UkR8MyLeCryR9MV68sCkQWJYAmw/lKDzr8x/J1VzfLRsPQ9ExNiyYaOIOChPX2N/SF92L1t1RVw11xcR90fEkcCrgbOAyyVtkH91fz4idiRdEfVe4G+rbGuNYw+MB1aR2lMaFhHPkM7G3l9ntnrv8xLglor93DAiPjKUOLIlwKaSxjYwX733ajBrfKaGcMytAU4gVs26ktYvG0YDPwQ+JGmypPWAfwNuj4gHSXXZb5Z0SJ73Y9T4VSppV0lvy7/6/kz6Yl+dJz9MquOu5XvAFyRNUvIWSZs1uE9nAqdIWp/UoP6kpE/nexdGSXqTpF3zvJcBp0naRNJWwAmDrLvu+iR9UFJfRLxIargFWC1pX0lvzvdFPEmqXlldZf0/Aj4paaKkDUnH/tJ46WqnoTgFOEbSyQPHTtJOki7J0+u9zz8HXifpaEnr5mFXSTsMNYhcbXk1cG4+zutK2qvKrIO9V4NZ4zM1hGNuDXACsWp+QbpyZWA4PSJuAP6JVOe9nHQmMBUgIh4BDge+TKru2JF01dFzVda9MakO+3FSVcmjwFfztPOBHXNVxU+qLPs10pf7taR//vNJDceNuCpv88O5rvxvgMmkq3oeISWnV+V5zyBVwT0AXA9cXmNfgHSWM8j6DgTmSXoa+AYwNSL+Qkqyl+d9WUBquP9BlU1cQKp6uTWv/y+k+y2GLCL+h9RQvh+wWNJjwAzSe84g7/NTpKvtppLOVP5IOqNar0gswNGkL/Dfka4u+0SVeAc7toP5EvC5/Jk6icaPuTXANxJa00lah/QFfFRE3NTueIZL0kdIX/p7tzsWs07iMxBrCknvljQ2V3t8hnSZ7W2DLNaRJI2TtKekdSS9HvhH4Mp2x2XWabrqLlrraHuQ6s/HAPOBQ/Ilrt1oDHAeMJHUZnEJ6TJgMyvjKiwzMyvEVVhmZlZIW6uwJB1IuiplFPC9qOgtVdIxwFdI16ADfCsivpenTSP1DwSp+4uZg21v8803jwkTJjQneDOzHjFnzpxHIqKvsrxtCSRfh30OcADpip3ZkmZFxPyKWS+NiBMqlt0U+BegRLpRaE5etu5drRMmTKC/v79p+2Bm1gsk/aFaeTursHYDFuX+fZ4nNVROaXDZdwPXRcRAVwjXka61NzOzEdLOBLIVa/YntDSXVXq/pLmSLld6QM1QlkXSdEn9kvpXrlzZjLjNzIz2JhBVKau8JOxnwISIeAvpjuCBdo5Glk2FETMiohQRpb6+l1XhmZlZQe1MIEtZs5O6rUndI/xVRDyau5SG1P3FWxtd1szMWqudCWQ2MCl3EDeG1L/OrPIZ8jMDBhxM6rsGUnfM78qdsG1C6p/nmhGI2czMsrZdhRURqySdQPriH0V6iM08SWcA/RExCzhR0sGkrqsfIz3Zjoh4TNIXSEkI4IyIeGzEd8LMrIf11J3opVIpfBmvmdnQSJoTEaXKct+JbmZmhTiBmFnzSGmwnuAEYmZmhTiBmJlZIU4gZmZWiBOImZkV4gRiZmaFOIGYmVkhTiBmZlaIE4iZWSv0wD0xTiBmZlaIE4iZmRXiBGJmZoU4gZiZWSFOIGbQEw2eZs3mBGJmZoU4gZiZWSFOIGZmVkhbE4ikAyUtlLRI0qlVpn9K0nxJcyXdIGnbsmmrJd2Vh1kjG7mZWZkebUMb3a4NSxoFnAMcACwFZkuaFRHzy2b7LVCKiGckfQT4MvCBPO3ZiJg8okEXNfDB6qHnz5vZ2q+dZyC7AYsiYnFEPA9cAkwpnyEiboqIZ/LobcDWIxyjmZnV0M4EshWwpGx8aS6r5Vjg6rLx9SX1S7pN0iGtCNDMzGprWxUWUK3CsGodj6QPAiVg77Li8RGxTNJ2wI2S7omI31dZdjowHWD8+PHDj9rMzID2noEsBbYpG98aWFY5k6T9gc8CB0fEcwPlEbEs/10M3AzsXG0jETEjIkoRUerr62te9MMx0ODWg41uZrb2aGcCmQ1MkjRR0hhgKrDG1VSSdgbOIyWPFWXlm0haL7/eHNgTKG98NzOzFmtbFVZErJJ0AnANMAq4ICLmSToD6I+IWcBXgA2B/1L6tf5QRBwM7ACcJ+lFUhI8s+LqLTMzazFFD11aWiqVor+/f+Q3XHkZb3nVVQ8d/47mS62bo1ePY7X9XouOhaQ5EVGqLPed6GZmVogTiA2dLwAwM5xAzMysoHbeB2KdaC2qt7UGuD3OhsFnIGZmVogTiHUHt7uYdRwnEDMzK8QJZG3QyK9z/4I3syZzArG1ixOl2YhxAjEzs0J8Ga+ZDa6Zl3e3Yl3NWp8Nic9ArPN0SjVUp8Rh1qGcQMzMulEH/MBxAjFrtg74xzYbCU4gZmZWiBvRrf26uf8tN+LW5+OzVvMZiJmZFeIzEOs9nXLG00gcjf6Cb3SfOmXfba3gBNIo/+OZWbN1eRVf26uwJB0oaaGkRZJOrTJ9PUmX5um3S5pQNu20XL5Q0rtHMu6m65Urd3plP62zNPNz147PcIf+37Q1gUgaBZwDvAfYEThS0o4Vsx0LPB4RrwXOBs7Ky+4ITAXeCBwInJvXZ7Z269AvE+s97T4D2Q1YFBGLI+J54BJgSsU8U4CZ+fXlwDslKZdfEhHPRcQDwKK8PjPrNk6KzTHCx7HdbSBbAUvKxpcCb6s1T0SskvQnYLNcflvFsltVbkDSdGA6wPjx44tHWq1+slq7SLWyymWrrauZ629knuHM18xYq5UN5/jUOhbl0xpdf9Fj0epj3cxtNjPWRtffzM9/s+OvLGvH56fo/021sha3sbT7DKRaqqzcy1rzNLIsETEjIkoRUerr6ysQopmZVdPuBLIU2KZsfGtgWa15JI0GXgU81uCyVk9EV175YVaXP9cjpt0JZDYwSdJESWNIjeKzKuaZBUzLrw8DboyIyOVT81VaE4FJwB0jFPfI8D+CmXWwtraB5DaNE4BrgFHABRExT9IZQH9EzALOB74vaRHpzGNqXnaepMuA+cAq4GMRsXqEd2BEN2dm1kkUPfQlWCqVor+/v7UbafUNh0O5e7lZMRTdZicdi2bFUXSf2nGneDfF2uj6W318GmlEb/Y2i8wz1O0Nc32S5kREqbK83VdhmZk1rod+8HaDdreBmJlZl/IZiFkn8S9s6yJOIGZmI2Ut+4HgBGLday37ZzTrNoO2gUjaQtL5kq7O4ztKOrb1oVnH8P0oZlZFI43oF5Hu09gyj98HfKJVAZmZjYhO/WHUqXFV0UgC2TwiLgNehHTzHzCyN+yZmVnHaSSB/FnSZuSOCiXtDvyppVGZmVnHa6QR/VOkfqe2l/RroI/UJ5V1qi45/bUW8+fAWmzQBBIRd0raG3g9qQv1hRHxQssjs+r8pWCdwp/FnjdoApH0txVFu0giIv6zRTGZmVkXaKQKa9ey1+sD7wTuBJxAzMx6WCNVWP9QPi7pVcD3WxaR2dqom6p7uilWa6sinSk+Q3p4k5mZ9bBG2kB+xkvPGl8H2BG4rJVBmZlZ52ukDeSrZa9XAX+IiKUtisfMzLpEI20gt4xEIGZm1l1qJhBJT/FS1dUak4CIiI1bFpWZmXW8mo3oEbFRRGxcZdhouMlD0qaSrpN0f/67SZV5Jkv6jaR5kuZK+kDZtIskPSDprjxMHk481gRd1AGcmTVHw1dhSXq1pPEDwzC3eypwQ0RMAm7I45WeAf42It4IHAh8XdLYsuknR8TkPNw1zHhsbTWQ2JzczJqukeeBHCzpfuAB4BbgQeDqYW53CjAzv54JHFI5Q0TcFxH359fLgBWkfrjMzKwDNHIG8gVgd+C+iJhIuhP918Pc7hYRsRwg/311vZkl7QaMAX5fVvzFXLV1tqT16iw7XVK/pP6VK1cOM2wzMxvQSAJ5ISIeBdaRtE5E3AQM2uYg6XpJ91YZpgwlQEnjSHe+fygiXszFpwFvIHWzsinw6VrLR8SMiChFRKmvzycwZmbN0sh9IE9I2hC4FbhY0grS/SB1RcT+taZJeljSuIhYnhPEihrzbQxcBXwuIm4rW/fy/PI5SRcCJzWwH2Zm1kSNnIFMAZ4FPgn8klSN9DfD3O4sYFp+PQ34aeUMksYAVwL/GRH/VTFtXP4rUvvJvcOMp3ncYGtmPaLefSDfAn4YEf9TVjyz1vxDdCZwmaRjgYeAw/M2S8DxEXEccASwF7CZpGPycsfkK64ultRHuiflLuD4JsVlZmYNUtT4tSzp48BUYBxwKfCjbr9ctlQqRX9/f7vD6B1S+uszMiuiHZ+fym0OjI90HM3SpPglzYmIUmV5vRsJvxERewB7A48BF0paIOmfJb2ucCRmZo1wdXDHG7QNJCL+EBFnRcTOwP8DDgUWtDwyMzPraI3cSLiupL+RdDHpBsL7gPe3PDIzM+to9RrRDwCOBP4vcAdwCTA9Iv48QrGZmVkHq3cfyGeAHwInRcRjIxSPmZl1iZoJJCL2HclAzMysuxR5JrqZmZkTiJmZFdPIVVhnNVJmZma9pZEzkAOqlL2n2YGYmbWdb14cknqX8X4E+CiwnaS5ZZM2YvjPAzEz63xOJnXVu4z3h6QbB7/Emo+cfcqX9ZqZdYEWJ8B6l/H+CfgTcKSkUcAWef4NJW0YEQ+1NDIzM+togz5QStIJwOnAw8DAEwEDeEvrwjIzs07XyBMJPwG8Pj/W1szMDGjsKqwlpKosMzOzv2rkDGQxcLOkq4DnBgoj4msti8rMzDpeIwnkoTyMyYOZmdngCSQiPg8gaYNmduUuaVPSo3InAA8CR0TE41XmWw3ck0cfioiDc/lEUhfzmwJ3AkdHxPPNis/MzOprpCuTPSTNJz+FUNJOks5twrZPBW6IiEnADax5r0m5ZyNich4OLis/Czg7L/84cGwTYjIzswY10oj+deDdwKMAEXE3sFcTtj0FmJlfzwQOaXRBSQL2Ay4vsryZmQ1fQ73xRsSSiqLVTdj2FhGxPK9/OfDqGvOtL6lf0m2SBpLEZsATEbEqjy8Ftqq2sKTpefn+lStXNiFsMzODxhrRl0h6OxCSxgAnkquzBiPpeuA1VSZ9tvEQGR8RyyRtB9wo6R7gySrzVb1nPyJmADMASqWSO7YxM2uSRhLI8cA3SL/wlwLXAh9rZOURsX+taZIeljQuIpZLGgesqLGOZfnvYkk3AzsDPwbGShqdz0K2BpY1EpOZmTXHoFVYEfFIRBwVEVtExKsj4oNNuit9FjAtv54G/LRyBkmbSFovv94c2BOYHxEB3AQcVm95MzNrnXrduZ8SEV+W9B9UqR6KiBOHue0zgcskHUu6z+TwvN0ScHxEHAfsAJwn6UVSsjszIubn5T8NXCLpX4HfAucPMx4zMxuCelVYA+0c/a3YcD6LeWeV8n7guPz6f4A311h+MbBbK2IzM7PB1evO/Wf578xa85iZWe9q5EbC6ySNLRvfRNI1rQ3LzMw6XSP3gfRFxBMDI7m7kVr3bJiZWY9oJIGsljR+YETSttS458LMzHpHI/eBfBb4laRb8vhewPTWhWRmZt2gkd54fylpF2B3QMAnI+KRlkdmZmYdrWYVlqQ35L+7AONJd3r/LzA+l5mZWQ+rdwbyKVJV1b9XmRak3nDNzKxH1Usg1+W/x+ab9szMzP6q3lVYp+W/l9eZx8zMelS9M5DHJN0EbCdpVuXEiqcDmplZj6mXQA4CdgG+T/V2EDMz62H1Esj5EXG0pO9GxC115jMzsx5Urw3krfmu86Ny/1eblg8jFaCZmXWmemcg3wF+CWwHzCHdRDggcrmZmfWommcgEfHNiNgBuCAitouIiWWDk4eZWY9r5JG2H5H0DkkfgvRoWUkTWx+amZl1skaeB/IvpMfHDtwXMgb4QSuDMjOzztdId+6HAgcDfwaIiGXARq0MyszMOl8jCeT5iAjyM0AkbTDcjeYrua6TdH/+u0mVefaVdFfZ8BdJh+RpF0l6oGza5OHGZGZmQ9NIArlM0nnAWEkfBq4HvjvM7Z4K3BARk4Ab8vgaIuKmiJgcEZNJHTc+A1xbNsvJA9Mj4q5hxmNmZkPUyPNAvirpAOBJ4PXAP0fEdYMsNpgpwD759UzgZlI7Sy2HAVdHxDPD3K6ZmTVJI2cgAHOBW0hf9Hc3YbtbRMRygPx3sGesTwV+VFH2RUlzJZ0tab1aC0qaLqlfUv/KlSuHF7WZmf1VI1dhHQHcARwOHAHcLumwBpa7XtK9VYYpQwlQ0jjgzcA1ZcWnAW8AdgU2pc7ZS0TMiIhSRJT6+vqGsmkzM6uj0Wei7xoRKwAk9ZHaQep28x4R+9eaJulhSeMiYnlOECvqrOoI4MqIeKFs3cvzy+ckXQic1MB+mJlZEzVShbXOQPLIHm1wuXpmAdPy62nAT+vMeyQV1Vc56SBJwCHAvcOMx8zMhqiRM5BfSrqGl77EPwBcPcztnkm6uutY4CFS9RiSSsDxEXFcHp8AbENqfyl3cT4TEnAXcPww4zEzsyFSusVjkJmk9wHvIH1h3xoRV7Y6sFYolUrR39/f7jB6h3L/mw18xsysc0maExGlyvKaZyCSXku6WurXEXEFcEUu30vS9hHx+9aFa2Zmna5eW8bXgaeqlD+Tp5mZWQ+rl0AmRMTcysKI6AcmtCwiMzPrCvUSyPp1pr2i2YGYmVl3qZdAZue+r9aQr5ya07qQzMysG9S7jPcTwJWSjuKlhFEiPQ/k0FYHZmZmna1mAomIh4G3S9oXeFMuvioibhyRyMzMrKM10hvvTcBNIxCLmZl1keF2SWJmZj3KCcTMzApxAjEzs0KcQMzMrBAnEDMzK8QJxMzMCnECMTOzQpxAzMysECcQMzMrxAnEzMwKaVsCkXS4pHmSXszPQq8134GSFkpaJOnUsvKJkm6XdL+kSyWNGZnIzcwM2nsGci/wPuDWWjNIGgWcA7wH2BE4UtKOefJZwNkRMQl4HDi2teGamVm5tiWQiFgQEQsHmW03YFFELI6I54FLgCmSBOwHXJ7nmwkc0rpozcysUqe3gWwFLCkbX5rLNgOeiIhVFeUvI2m6pH5J/StXrmxpsGZmvWTQ7tyHQ9L1wGuqTPpsRPy0kVVUKYs65S8vjJgBzAAolUpV5zEzs6FraQKJiP2HuYqlwDZl41sDy4BHgLGSRuezkIFyMzMbIZ1ehTUbmJSvuBoDTAVmRUSQHnJ1WJ5vGtDIGY2ZmTVJOy/jPVTSUmAP4CpJ1+TyLSX9AiCfXZwAXAMsAC6LiHl5FZ8GPiVpEalN5PyR3gczs16m9GO+N5RKpejv7293GL1Duamqhz5jZmsjSXMi4mX363V6FZaZmXUoJxAzMyvECcTMzApxAjEzs0KcQMzMrBAnEDMzK8QJxMzMCnECMTOzQpxAzMysECcQMzMrxAnEzMwKcQIxM7NCnEDMzKwQJxAzMyvECcTMzApxAjEzs0KcQMzMrBAnEDMzK8QJxMzMCmlLApF0uKR5kl6U9LLn7OZ5tpF0k6QFed6Pl007XdL/SrorDweNXPRmZgYwuk3bvRd4H3BenXlWAf8YEXdK2giYI+m6iJifp58dEV9tdaBmZlZdWxJIRCwAkFRvnuXA8vz6KUkLgK2A+TUXMjOzEdMVbSCSJgA7A7eXFZ8gaa6kCyRtUmfZ6ZL6JfWvXLmyxZGamfWOliUQSddLurfKMGWI69kQ+DHwiYh4Mhd/G9gemEw6S/n3WstHxIyIKEVEqa+vr+DemJlZpZZVYUXE/sNdh6R1Scnj4oi4omzdD5fN813g58PdlpmZDU3HVmEpNZCcDyyIiK9VTBtXNnooqVHezMxGULsu4z1U0lJgD+AqSdfk8i0l/SLPtidwNLBflct1vyzpHklzgX2BT470PpiZ9bp2XYV1JXBllfJlwEH59a+AqpdpRcTRLQ3QzMwG1bFVWGZm1tmcQMzMrBAnEDMzK8QJxMzMCnECMTOzQpxAzMyskHb1xmu9IKLdEZhZC/kMxMzMCnECMTOzQpxAzMysECcQMzMrxAnEzMwKcQIxM7NCnEDMzKwQJxAzMyvECcTMzApR9NDdwpJWAn9ocPbNgUdaGE6rOf72cvzt5fiba9uI6Kss7KkEMhSS+iOi1O44inL87eX428vxjwxXYZmZWSFOIGZmVogTSG0z2h3AMDn+9nL87eX4R4DbQMzMrBCfgZiZWSFOIGZmVogTSBWSDpS0UNIiSae2O57BSLpA0gpJ95aVbSrpOkn357+btDPGeiRtI+kmSQskzZP08VzeFfsgaX1Jd0i6O8f/+Vw+UdLtOf5LJY1pd6z1SBol6beSfp7HuyZ+SQ9KukfSXZL6c1lXfH4AJI2VdLmk3+X/gz26IX4nkAqSRgHnAO8BdgSOlLRje6Ma1EXAgRVlpwI3RMQk4IY83qlWAf8YETsAuwMfy8e8W/bhOWC/iNgJmAwcKGl34Czg7Bz/48CxbYyxER8HFpSNd1v8+0bE5LL7J7rl8wPwDeCXEfEGYCfS+9D58UeEh7IB2AO4pmz8NOC0dsfVQNwTgHvLxhcC4/LrccDCdsc4hH35KXBAN+4D8ErgTuBtpDuJR+fyNT5XnTYAW5O+pPYDfg6oy+J/ENi8oqwrPj/AxsAD5Iuauil+n4G83FbAkrLxpbms22wREcsB8t9XtzmehkiaAOwM3E4X7UOu/rkLWAFcB/weeCIiVuVZOv1z9HXgFODFPL4Z3RV/ANdKmiNpei7rls/PdsBK4MJchfg9SRvQBfE7gbycqpT5WucRIGlD4MfAJyLiyXbHMxQRsToiJpN+ye8G7FBttpGNqjGS3gusiIg55cVVZu3I+LM9I2IXUtXzxyTt1e6AhmA0sAvw7YjYGfgznVhdVYUTyMstBbYpG98aWNamWIbjYUnjAPLfFW2Opy5J65KSx8URcUUu7qp9AIiIJ4CbSW05YyWNzpM6+XO0J3CwpAeBS0jVWF+ne+InIpblvyuAK0lJvFs+P0uBpRFxex6/nJRQOj5+J5CXmw1MylegjAGmArPaHFMRs4Bp+fU0UrtCR5Ik4HxgQUR8rWxSV+yDpD5JY/PrVwD7kxpBbwIOy7N1bPwRcVpEbB0RE0if9xsj4ii6JH5JG0jaaOA18C7gXrrk8xMRfwSWSHp9LnonMJ8uiN93olch6SDSL7BRwAUR8cU2h1SXpB8B+5C6gH4Y+BfgJ8BlwHjgIeDwiHisXTHWI+kdwH8D9/BSHfxnSO0gHb8Pkt4CzCR9XtYBLouIMyRtR/pFvynwW+CDEW5/YxkAAAPQSURBVPFc+yIdnKR9gJMi4r3dEn+O88o8Ohr4YUR8UdJmdMHnB0DSZOB7wBhgMfAh8meJDo7fCcTMzApxFZaZmRXiBGJmZoU4gZiZWSFOIGZmVogTiJmZFeIEYms9Sa+RdImk30uaL+kXkl5XcF0n5t5SL5a0nqTrcw+wH8hdUNTseFPSwUV7d869tX60zvSnh7i+fQZ63TUravTgs5h1r3yT4pXAzIiYmssmA1sA9xVY5UeB90TEA7nH3XVzFyYAl9ZbMCJmUfym1LF52+cWXN6s6XwGYmu7fYEXIuI7AwURcVdE/LeSr0i6Nz9L4gMD80g6WdJsSXPLnu/xHVLHd7MkfRr4ATA5n4FsL+lmSaU874GS7lR6RsgNuewYSd/Kr/sk/ThvY7akPXP56UrPd7lZ0mJJJ+aQzgS2z9v6Sq2dzWcWN5c9W+LinEQHYvqdpF8B7ytbZoO8zdm5M78pufxTki7Ir9+cj9Mrh/d22NrEZyC2tnsTMKfGtPeRnt+xE+ku/tmSbgXeDEwi9ackUsLYKyKOl3Qg6bkTj0i6nXzXNkD+nkZSH/BdYK98prJplW1/g/SsjV9JGg9cw0sdML6BlPg2AhZK+japc703lZ3t1LMz8EZS31W/BvZUesjSd0n9XC1izbOlz5K6L/m73CXLHZKuJ/XGcLOkQ/M8fx8RzzSwfesRTiDWy94B/CgiVpM6rrsF2BXYi9Sf0m/zfBuSEsqtDa53d+DWiHgAoEb3E/sDOw4kHWDjgf6cgKtylyHPSVpBqm4bijsiYimAUhfzE4CngQci4v5c/gNgoNvzd5E6Uzwpj68PjI+IBZKOAeYC50XEr4cYh63lnEBsbTePlzoErFSty/KB8i9FxHkFtykG7/p8HWCPiHh2jQVTQinvb2o1Q/8/rbV8rZgEvD8iFlaZNomUfLYcYgzWA9wGYmu7G4H1JH14oEDSrpL2Jp1RfEDpYVB9pDOPO0jVSX+n9HwSJG0laSgP8/kNsLekiXn5alVY1wInlMU0WNXUU6QqraJ+B0yUtH0eP7Js2jXAP5S1leyc/76KVNW2F7CZpFqJ2HqUE4it1SL1FnoocEC+jHcecDqpfeBKUvXM3aREc0pE/DEirgV+CPxG0j2k5zM0/OUdEStJ1UNXSLqb6ldnnQiUciP9fOD4Qdb5KPDr3JBdsxG9zvJ/yTFdlRvR/1A2+QvAusBcSffmcYCzgXMj4j7S89DPHGIitbWce+M1M7NCfAZiZmaFOIGYmVkhTiBmZlaIE4iZmRXiBGJmZoU4gZiZWSFOIGZmVsj/B08BAc8TovpGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use a large C to disable regularization\n",
    "reg_full = linear_model.LogisticRegression(C=100000000, solver='newton-cg', penalty='l2')\n",
    "\n",
    "reg_full.fit(X_training, Y_training)\n",
    "\n",
    "# print the coefficients from the logistic regression model.\n",
    "print(\"Coefficients obtained using the entire training set: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( reg_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "reg_coef = reg_full.coef_.reshape(reg_full.coef_.shape[1],)\n",
    "plt.figure()\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.45       # the width of the bars\n",
    "plt.bar(ind, reg_coef, width, color='r')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.037933776545071705]\n",
      "[0.037933776545071705, -0.037933776545071705]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713, -0.10458075696262054]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713, -0.10458075696262054, 0.027718922948841835]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713, -0.10458075696262054, 0.027718922948841835, 0.0915767840936177]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713, -0.10458075696262054, 0.027718922948841835, 0.0915767840936177, 0.0102057574422204]\n",
      "[0.037933776545071705, -0.037933776545071705, -0.27637114328432105, 0.27637114328432105, 0.23970660533597085, 0.028623052558015828, -0.02862305255859232, -0.1403771022052074, 0.1403771022059963, -0.13457887361337018, 0.13457887361360868, -0.017673975426403265, 0.17298297616351554, -0.1010396769885656, 0.18737985757334724, -0.2280227322155696, 0.02460383196677438, 0.06148619395386828, 0.10006409831836899, 0.15205703327313297, -0.0898819631684546, 0.12992073145568864, -0.23434267085101337, 0.04439876331088488, 0.3155328300117487, 0.10007159950074973, -0.2985456619825648, 0.06831822750569351, 0.08205830366634063, -0.17513180215269972, 0.04964659162279869, -0.06897475436461938, 0.04317070794108046, 0.020129277434840793, 0.04485623639953481, -0.13782984030433243, 0.04528973801800339, 0.17407052685995317, -0.1725594174089823, 0.10543026106492054, -0.14655685796041848, 0.43395687944353656, 0.06704965945134116, 0.10916135680086843, 0.013695861675716773, -0.3100402080549638, -0.051577363174336116, 0.11002732275480999, -0.018977875244733603, 0.031418527528298074, -0.10611657955725273, -0.3907211446796236, -0.3968222337953397, 0.028433067444849493, 0.284569254467249, 0.36509298692063535, -0.07834110874940518, 0.04100620793233713, -0.10458075696262054, 0.027718922948841835, 0.0915767840936177, 0.0102057574422204, -0.40372468854573207]\n",
      "Number of coefficients between -1 and 1:  63\n"
     ]
    }
   ],
   "source": [
    "near_zero_coeff = []\n",
    "for i in range(len(reg_full.coef_[0])):\n",
    "    if reg_full.coef_[0,i] < 1 and reg_full.coef_[0,i] > -1:\n",
    "        near_zero_coeff.append(reg_full.coef_[0,i])\n",
    "        print(near_zero_coeff)\n",
    "print(\"Number of coefficients between -1 and 1: \", len(near_zero_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for training dataset: 0.775\n",
      "Accuracy for test dataset: 0.6345381526104418\n"
     ]
    }
   ],
   "source": [
    "# prediction on training data\n",
    "Y_training_prediction_LR = reg_full.predict(X_training)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "\n",
    "print(\"Accuracy for training dataset:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR))\n",
    "\n",
    "# prediction on test data\n",
    "Y_test_prediction_LR = reg_full.predict(X_test)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "\n",
    "print(\"Accuracy for test dataset:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: 0.7475\n",
      "Best value of C: [0.00599484]\n",
      "[[0.56097561 0.56097561 0.75609756 0.7804878  0.75609756 0.75609756\n",
      "  0.73170732 0.73170732 0.73170732 0.73170732]\n",
      " [0.56097561 0.6097561  0.6097561  0.56097561 0.48780488 0.46341463\n",
      "  0.46341463 0.46341463 0.46341463 0.46341463]\n",
      " [0.56097561 0.58536585 0.65853659 0.70731707 0.68292683 0.65853659\n",
      "  0.65853659 0.65853659 0.65853659 0.65853659]\n",
      " [0.56097561 0.6097561  0.68292683 0.63414634 0.58536585 0.58536585\n",
      "  0.58536585 0.58536585 0.58536585 0.58536585]\n",
      " [0.56097561 0.58536585 0.75609756 0.75609756 0.73170732 0.73170732\n",
      "  0.73170732 0.73170732 0.73170732 0.73170732]\n",
      " [0.56410256 0.58974359 0.74358974 0.71794872 0.74358974 0.74358974\n",
      "  0.74358974 0.74358974 0.74358974 0.74358974]\n",
      " [0.56410256 0.58974359 0.61538462 0.69230769 0.66666667 0.69230769\n",
      "  0.69230769 0.69230769 0.69230769 0.69230769]\n",
      " [0.56410256 0.58974359 0.79487179 0.79487179 0.74358974 0.74358974\n",
      "  0.74358974 0.74358974 0.74358974 0.74358974]\n",
      " [0.56410256 0.58974359 0.64102564 0.53846154 0.56410256 0.56410256\n",
      "  0.56410256 0.56410256 0.56410256 0.56410256]\n",
      " [0.56410256 0.58974359 0.69230769 0.66666667 0.69230769 0.69230769\n",
      "  0.69230769 0.69230769 0.69230769 0.69230769]]\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.7948717948717948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([0.00599484]), class_weight=None, cv=10,\n",
       "                     dual=False, fit_intercept=True, intercept_scaling=1.0,\n",
       "                     l1_ratios=None, max_iter=100, multi_class='warn',\n",
       "                     n_jobs=None, penalty='l2', random_state=None, refit=True,\n",
       "                     scoring=None, solver='newton-cg', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the model using LogisticRegressionCV passing an appropriate solver, cv value, and choice of penalty\n",
    "regL2 = linear_model.LogisticRegressionCV(solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "\n",
    "regL2.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( regL2.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_prediction = regL2.predict(X_training)\n",
    "\n",
    "CV_accuracies = metrics.accuracy_score(Y_training, CV_prediction)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of C: {}\".format( regL2.C_ ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "print(regL2.scores_[1])\n",
    "regL2_best_CV_accuracy = np.max(regL2.scores_[1])\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( regL2_best_CV_accuracy ))\n",
    "\n",
    "#define the model using the best C and an appropriate solver\n",
    "\n",
    "regL2_best = linear_model.LogisticRegressionCV(Cs=regL2.C_, solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model using the best C on the entire training set\n",
    "\n",
    "regL2_best.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set without regularization: [[ 0.03793378 -0.03793378 -0.27637114  0.27637114  0.23970661  0.02862305\n",
      "  -0.02862305 -0.1403771   0.1403771  -0.13457887  0.13457887 -0.01767398\n",
      "   0.17298298 -0.10103968  0.18737986 -0.22802273 -1.35823587  0.02460383\n",
      "   0.06148619  0.1000641   0.15205703 -0.08988196  0.12992073 -0.23434267\n",
      "   0.04439876  0.31553283  0.1000716  -0.29854566  0.06831823  0.0820583\n",
      "  -0.1751318   0.04964659 -0.06897475  0.04317071  0.02012928  0.04485624\n",
      "  -0.13782984  0.04528974  0.17407053 -0.17255942  0.10543026 -0.14655686\n",
      "   0.43395688  0.06704966  0.10916136  0.01369586 -0.31004021 -0.05157736\n",
      "   0.11002732 -0.01897788  0.03141853 -0.10611658 -0.39072114 -0.39682223\n",
      "   0.02843307  0.28456925  0.36509299 -0.07834111  0.04100621 -0.10458076\n",
      "   0.02771892  0.09157678  0.01020576 -0.40372469]]\n",
      "Coefficients obtained using the entire training set with regularization: [[-0.00311754  0.00311754 -0.13248272  0.13248272  0.08854747  0.00331037\n",
      "  -0.00331037 -0.04627941  0.04627941 -0.05588841  0.05588841 -0.00613819\n",
      "   0.01920133 -0.04820673  0.05747982 -0.02062846 -0.09296312  0.01002816\n",
      "  -0.0102438   0.02103897  0.00377505 -0.00808828  0.0208535  -0.04307616\n",
      "   0.02809867  0.02378713  0.00945926 -0.06848412  0.00648477  0.03904018\n",
      "  -0.04538114  0.00647417 -0.03298413  0.05077197 -0.00140486  0.0215104\n",
      "  -0.03397046 -0.00013762  0.08538196 -0.06412995  0.0141512  -0.05791535\n",
      "   0.0758299   0.01167084  0.03503756  0.01143042 -0.07717115 -0.03169407\n",
      "   0.00201568 -0.03428324  0.04084049  0.01886782 -0.09675789 -0.13172135\n",
      "   0.00884452  0.0854388   0.10886448 -0.03300714  0.01243546 -0.03166721\n",
      "  -0.00252638  0.0426799   0.03966415 -0.11870645]]\n",
      "Intercept: [-0.25007383]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAEWCAYAAAAO4GKjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3debxVZdn/8c9XRDFBQURDUcFyzAHtQFjmkHMlak9OqWEOZMVPG50qs7KnTCvtyXoyNWfRxyEpK0fULKdDEak45YiYIGhqiopevz/u++Bms6cz7uF836/Xfu295mutda917XuNigjMzMya3XL1DsDMzKwnOKGZmVlLcEIzM7OW4IRmZmYtwQnNzMxaghOamZm1hKZNaJIOknRDF4e9X9IOPRxSw5P0B0mT6h1HZ0haSdJvJf1b0v/ldqdIel7SvyStK+kVSQOqjOfDkh7qm6ibg6QdJM3pwfGNlhSSlu+pcVaY1smSLu7t6XSWpEMl3dGN4U+UdE4Px9Sj67k78ra6fm+Nv08SmqQnJO3ck+OMiEsiYtcapn2+pFOKhn1fRNzamekVbKyv5M8Tko7vZNh1FRF7RMQFvTFuSeMl/V7Si5IWSrpH0md6YNSfBNYEhkfEvpLWAb4CbBoR746IpyJicES8VWkkEfGniNioB+Lp0fIsaVtJf8kJe6GkP0sal7t1a+fYX+Ud+Nt5O31Z0kM9VBZ7XUT8d0Qc0VfTk/SgpMNKtD9GUntPTy9vq4/19Hg7NG0NrY6GRsRg0o72m5J26ekJ9MU/3J4kaRvgFuA24L3AcOBzwB49MPr1gIcjYnFB84KImNcD464rSasAvwP+B1gNWBv4NvB6PeOqRROU0bl5O10F+BLwK0k98oemt9RpmV4AfLpE+0Nyt06pe7mIiF7/AE8AO5fpdiTwKLAQmAasVdBtV+Ah4N/Az0k7zCNyt0OBO/JvAT8B5uV+ZwGbAZOBN4E3gFeA3xbHAwwATgT+CbwMzADWKRHnaCCA5Qva3QN8raB5LeAqYD7wOHB0QbeVSAXkBWA2cCwwp2gZHZdjfx1Yvsr4xgPtwEvAc8CPc/tBwMXAAuBF4F5gzdzt1oLltxzwDeDJvNwuBFYtmtdJwFPA88DXK6zfO4CzqpSBSut5Y+DG3O0hYL/c/tt53b2Z199ngdeAt3Pz+cXrhZQYfg3Mzcv6N7n9DkXLu9KyPRm4Ii+Tl4H7gbbc7aI8/ddyDMdWWuZVlkkb8GKZbpsAi4C38nRezO0/Bvwtr/engZNLlNGS641UBs/Py+UB4GtFy+R43tkOHgD2Keh2KPBn0na2EDiFtO2cnqfzGPAFiraRonmqNv478vheyOtkj4LuY0jb/8u5rPwMuLjMdJZa17ndPGDfamUudxsO/DYv43vzvN5RtIwL9wO3UmK/lJvPzOvpJdK+5cNF5ezKXHZeAo7I7S7O3X+W133HZ3HH+qb6vqbsei5aLqPyeNcrKntvAKvn5lWBc4FngWc61n2FcvHevK7+ncvG5QXjDuC9BeO9MM/Dk6T90XK1lIey21S1HnriQ5mEBnwkz/DWwIqkf6q3526r55X8CdLO/RjSjq1UQtstF5ahpOS2CTAydzsfOKVcPHll/wPYKA+7JenwVsWEBkwAXiVvlKQEMQM4CVgBWJ+0ke+Wu/8gr+RhuRDNYtmENhNYJxfIauO7Ezgk/x4MTMi/P0vaGN9F2uG8H1ilxIZ3GCnBrJ+Hvxq4qGhef5Vj2ZKUZDcpsVzeRdrp7lhh/VdazyuTNvjP5PW8de73fQUb/cUF49qhaLkVr5frgMvzch4IbF88XA3L9mRSMvloXobfB+4qV56rLPPjgd+VWS6rkJLgBaTa7LCi7odSsHMsmI/N8zxsQfozs3ct641UBv9ESvrrAPcVLct9STvK5YD9gf/wznZ0KGnH9//yeloJOAp4MI9rNWA6lRNatfG/SfrjM4BUw58LqKC8/5hUfrYjJbaqCS1PayLpT8hWNZa5qfnzLmDT3G9XE9rBpAS5POlQ+b+AQQXl7E1g7xznShSV94LxjCXt+Leitn1N2fVcYtw3At8oaP4++Y9gbv4N8Mu83NYg/ZH/bIVycRnw9RznIGDbgnEVJrQLgWuBIXm5PgwcXkt5KDsvtSal7nwon9DOBX5Y0Dw4z8RoUjX4zoJuygWrVEL7SF4YE8gZvmC486mc0B4C9qphHkbnlfEi6d95kP49dGxwHwCeKhrmBODX+feSApebj2DZhHZYQXO18d1OqsGsXtTPYcBfgC1KzMOtBcvvZuDzBd02yst++YJ5HVXQ/R7ggBLjXDv3u3GFZVdpPe8P/Kmo/18C3yrY6GtKaMBI0o5rWIkYlgxXw7I9GbipoNumwGvlynOlZV5Dudokl9E5pB3DNN6pUR9KUUIrMfwZwE+KlkXJ9ZbL4O4F3SZTeUc3k7xt5FiKl9ktwFEFzbtSIaHVMP5HC7q9K4/r3cC6edmsXND9UiontLdJ2+rrpD9cXyzoXrbMkXaebwIbFXTrcg2tRGwvAFsWlLPbi7qfXDxfwIhc5jrWYy37ms6s54OBh/Lv5Ui1+44/6mvmZbhSQf8HAtMrlIsLgbMLy2FBtyDV4Abk8W5a0O2zwK3VykOlMlXvc2hrkaqaAETEK6R/rGvnbk8XdAvSRr+MiLiFVD0/C3hO0tn5/EQt1iEdBqnV6qQd8ldJG87A3H49YK18UcSLkl4kHcpcM3dfan6KfpdqV218hwMbAg9KulfSx3P7i4DrgamS5kr6oaSBLGupZZ9/L18wfkj/Jju8mue72AukncfIEt1KTqtoPa8HfKBoPg8i7cg6ax1gYUS8UKW/assWlp33QRXOD9S6zJcREbMj4tCIGEU6TL4WKUmVJOkDkqZLmi/p36Ra0upFvZVbb8VlsHD9I+nTkmYWLJPNisZdXGYrjq9E7NXGvyTuiHg1/xycp/NCRPyn1mmRzqENJdWCf0r609uhUpkbQdoOqm2rNZH0FUmz80U/L5IOs1VapsXDDyQdlrw0IqYWxN+ZfU21ZXU1MFLSBNI+7V2kIx0d0xoIPFswrV+Samrl5uFYUgXknnxF+TIXnZCWwQosuw9au6C5XHkoq94JbS5pgQEgaWVS9fwZ0vHaUQXdVNhcLCJ+GhHvB95H2tF/raNTlRieBt7TmaAj4q2I+BHpsNTnC8bzeEQMLfgMiYiP5u5LzQ9p57vMqIviKju+iHgkIg4kFaxTgSslrRwRb0bEtyNiU+CDwMcpfdJ3qWXPO/+Cn+vEougoaHcC/1Wht0rr+WngtqL5HBwRn+tMHNnTwGqShtbQX6V1Vc1SZaoTy7zySCMeJNXWNis1nexSUi1unYhYFfhf0s6jFs+ydLlbt+OHpPVIhyqnkA65DyUdqiocd3E8ZcdXrMbxV4p7WC43VadVKCJeJ52b3lzS3rl1pTI3n7QdlNtWO5LquwralfzzJenDedr7kY4aDCWdV6q0TIv9D+nw6jcK2tWyr6lpvcCSbfhKUpk9BJgaEW8UTOt10pGgjmmtEhHvKzcPEfGviDgyItYi1bp+Lum9RZN9nlQTLt4HPVMp1mr6MqENlDSo4LM8aeP8jKSxklYE/hu4OyKeIP1D2FzS3rnfL1C+4IzL/1wHkgpcx8l0SDvoSvc9nAN8V9IGSraQNLzGefoBcKykQaRDOy9JOk7p3qkBkjbruASbdJHBCZKGSVqbtGFXUnF8kg6WNCIiOg6tALwlaUdJmyvdl/USqdCUuqT9MuBLksZIGkxa9pfHO1cTdsaxwKGSvtax7CRtKanjH2Wl9fw7YENJh0gamD/jJG3S2SAi4lngD6QNaFge13Yleq22rqpZqkx1YpkvRdLG+R/8qNy8Dulwzl0F0xklaYWCwYaQaqGLJI0HPlVjzLB0GRxFOu/RYWXSjml+juUzvJNYK43vaEmjJA0jnS8spyvjByAiniRdAPVtSStI2hbYs5Zh8/BvAD8inXOCCmUu0u0fVwMnS3qXpI0p+HMSEfNJO92Dc7k5jPJ/iIeQkuN8YHlJJ5FqjDWR9Flge+BTeTvv0Jl9TfF6LucC0qHY/6Lg6sa8Td0A/EjSKpKWk/QeSdtXiHvfjjJNOoITFG0PeTlfAXxP0pD8h+fLpAtkuqwvE9rvSeeeOj4nR8TNwDdJV+s8SyoYBwBExPOkk8g/JB2e2pRUqEtd0rwK6d/fC6Rq6wLS+S1I5282zdXl35QY9sekBXsDaWd0LunEZi2uy9M8Mq+gPUknbx8n/QM5h3SIAeA7pEOmjwM3kf4Rlb08u4bx7Q7cL+kV0pVUB0TEIlLSvzLPy2zShSilCsl5pENlt+fxL6K2gl8q1r+QDul8BHhM0kLSMfTf5+6V1vPLpHMvB5Bqcv8i1ThX7EospH+Yb5IuVpgHfLFEvNWWbTXfB76Ry9RXqbDMlW6U/UOZ8bxMOh9yt6T/kBLZfaSLByCdo7of+Jek53O7zwPfkfQyaQd9RY0xQzrn+iRpnm8grX8AIuIB0k7/TlIi3Zx09VolvyIdav078FdSIiipi+Mv9CnSslpIOtd1YSeGhVTe15W0Zw1lbgqpLPyLtIwuY+lt9UjSEaAFpCNCfykzzetJf7AeJi33RXTu8OWBpD9Oc/XO/a8n1lB+y67nCm4n1R6fiYh7i7p9mnR48AHS/u5KKp9iGEcq06+QjiYcExGPl+jv/5EqII+Rrmi8lLSeuqzjgoaGJ2k5UkI4KCKm1zue7pL0OVISKvtPx8zqT9KppIsRJtU7Fqus3ufQKpK0m6Sh+TDViaRjz3dVGawhSRop6UO5yr4R6V/4NfWOy8yWlg8Fb5FPQYwnXYDlbbUJNPrd/tuQqqEd1d29I+K1+obUZSuQrg4aQzrnNZV0s7iZNZYhpMOMa5EOW/+IdL+UNbimOeRoZmZWSUMfcjQzM6tVXQ85StqddIXeAOCciPhBUfdDgdN4596En0XEObnbJN65N+OUqOEp8quvvnqMHj26Z4I3M+snZsyY8XxEjKh3HNXULaHle3bOAnYhXb14r6Rp+fLeQpdHxJSiYVcjXbrbRrrHYUYetuITIkaPHk17e4+/EcHMrKVJqva0kYZQz0OO40nP6nos3/g4FdirxmF3A26MiI7HHN1Iui/LzMz6qXomtLVZ+ibDOSz9HK8O/yVplqQr85MUOjMskiZLapfUPn/+/J6I28zMGlA9E1qpZ7gVX3L5W2B0RGxBerpGx3myWoZNLSPOjoi2iGgbMaLhDwGbmVkX1TOhzWHpB2iOIj2GZomIWJAfLgrpMTvvr3VYMzPrX+qZ0O4FNsgPx12B9Fy1aYU9SCp8XthE0nPyID0jbdf88M1hpOeyXd8HMZuZWYOq21WOEbFY0hRSIhoAnBcR90v6DtAeEdNIT/KeSHpi9ULSS9+IiIWSvktKigDfiYiFfT4TZmbWMPrVk0La2trCl+2bmXWOpBkR0VbvOKrxk0LMzKwlOKGZWc+Q0sesTpzQzMysJTihmZlZS3BCMzOzluCEZmZmLcEJzczMWoITmpmZtQQnNDMzawlOaGZmPc3349WFE5qZmbUEJzQzM2sJTmhmZtYSnNDMzKwlOKGZgU/im7UAJzQzM2sJTmhmZtYSnNDMzKwl1DWhSdpd0kOSHpV0fInuX5b0gKRZkm6WtF5Bt7ckzcyfaX0buZlZ5hebNozl6zVhSQOAs4BdgDnAvZKmRcQDBb39DWiLiFclfQ74IbB/7vZaRIzt06C7SoKIekdhZtbS6llDGw88GhGPRcQbwFRgr8IeImJ6RLyaG+8CRvVxjGZm1iTqmdDWBp4uaJ6T25VzOPCHguZBktol3SVp794I0MzMmkfdDjkCpQ46lzwuJ+lgoA3YvqD1uhExV9L6wC2S/hER/ywx7GRgMsC6667b/ajNzKwh1bOGNgdYp6B5FDC3uCdJOwNfByZGxOsd7SNibv5+DLgV2KrURCLi7Ihoi4i2ESNG9Fz03eGTyGZmPa6eCe1eYANJYyStABwALHW1oqStgF+Sktm8gvbDJK2Yf68OfAgovJjEzMz6mbodcoyIxZKmANcDA4DzIuJ+Sd8B2iNiGnAaMBj4P6UazVMRMRHYBPilpLdJSfkHRVdHmplZP6PoR5eTt7W1RXt7e99PuPiy/Y7Djf1o2Tc831rRff21XJea7xYrT5JmRERbveOoxk8KMTOzluCEZp3ni1rMrAE5oZmZWUtwQrOluebVv3h9WwtxQjMzs5bghGbNwTUJM6vCCc3MzFqCE1of0beFvt0LtYxarzj0lYlm1uKc0Kx1OGmb9WtOaGZm1hKc0Myssp6s+fZ0LbqBa+S9dprBynJCs8bTCIcOGyEGM+sUJ7Q68j84M+sS/9kqyQnNrKc12c7Gf6x6hpdh/TmhmVmP8o7d6sUJzeqr2c9VNXv8vc3LxvqQE1o/05P/nv1P3MwaiRNai/B5kE5ohFpVJ5/wUnHd+mkx/ZK3+WU5odXKOwPrBO9sKqvH8mm49dEL+5T+Xu7qntAk7S7pIUmPSjq+RPcVJV2eu98taXRBtxNy+4ck7daXcfe4/pAwG2weizf8/r4zaFmNfGN4o06zSdU1oUkaAJwF7AFsChwoadOi3g4HXoiI9wI/AU7Nw24KHAC8D9gd+Hken3VTo+zYuxpDo8Tfa8rs3Fp+vnuBl1drqXcNbTzwaEQ8FhFvAFOBvYr62Qu4IP++EthJknL7qRHxekQ8Djyax2cNrpl2vLXG2h+Sb1djrTpcwXnCVl8WvTYu1+IAWL7O018beLqgeQ7wgXL9RMRiSf8Ghuf2dxUNu3bxBCRNBiYDrLvuul2PNGLZdh0FqLCbhE7Orb8VS7ovNXTuv1S7To+/1LjycLX0VzXWToyro78l46ol1tyuY4Ou1t9SOrksCuex1PirrqNal0XxuHohVko01xpr8bC9HWutZb/T5alg2FLD1RJ/VNq2OrG+S7Xr6eW6lK6W1xZX74RW6i9F8VIv108twxIRZwNnA7S1tbX+Gu2kRinkXY2jUeI3s/qr9yHHOcA6Bc2jgLnl+pG0PLAqsLDGYa2SUv/8ujtKJxirpyhTE7V+od4J7V5gA0ljJK1AushjWlE/04BJ+fcngVsiHSeYBhyQr4IcA2wA3NNHcfcNb5xmZjWr6yHHfE5sCnA9MAA4LyLul/QdoD0ipgHnAhdJepRUMzsgD3u/pCuAB4DFwBci4q0+noE+nZz1Dz1Zy221GnOt89Oo893s8Te6ep9DIyJ+D/y+qN1JBb8XAfuWGfZ7wPd6NcAGVI/C3uwbWCPE3wgxmLWyuic06z3egVpv69My5iMiVoUTmpktw3+GrBk5oZl1Uq/t7F0DMeuWel/laGZWVkvVFP2Hpde5hma9qld3SN5BmFmBqglN0prAfwNrRcQe+aHA20TEub0endWfk4b1Iy1VI+yHajnkeD7pPrG1cvPDwBd7KyAzs17XqA8taNS4mkQthxxXj4grJJ0AS26G7tsbmM3MrEv6U62zlhrafyQNJz/4V9IE4N+9GpWZmVkn1VJD+zLpuYnvkfRnYATpmYpWRt3/EfmQhbkM9Ji6b89Ws6oJLSL+Kml7YCPSK1seiog3ez0yK807KmsELofWgGq5yvHTRa22lkREXNhLMZmZmXVaLYccxxX8HgTsBPwVcEIzM7OGUcshx/9X2CxpVeCiXovIrBU10yG6ZorVrEBXHn31KullmmZmZg2jlnNovyVfsk9KgJsCV/RmUGZmZp1Vyzm00wt+LwaejIg5vRSPmZlZl9RyDu22vgjEzMysO8omNEkv886hxqU6ARERq/RaVGZmZp1U9qKQiBgSEauU+AzpbjKTtJqkGyU9kr+HlehnrKQ7Jd0vaZak/Qu6nS/pcUkz82dsd+KxHuCHqppZndV8laOkNSSt2/Hp5nSPB26OiA2Am3NzsVeBT0fE+4DdgTMkDS3o/rWIGJs/M7sZj7UqJ1qzfqNqQpM0UdIjwOPAbcATwB+6Od29gAvy7wuAvYt7iIiHI+KR/HsuMI/0HEkzM7Nl1FJD+y4wAXg4IsaQnhTy525Od82IeBYgf69RqWdJ44EVgH8WtP5ePhT5E0krVhh2sqR2Se3z58/vZthmZtaoaklob0bEAmA5SctFxHSg6jkrSTdJuq/EZ6/OBChpJOnJJJ+JiLdz6xOAjUmP5VoNOK7c8BFxdkS0RUTbiBGu4JmZtapa7kN7UdJg4HbgEknzSPejVRQRO5frJuk5SSMj4tmcsOaV6W8V4DrgGxFxV8G4n80/X5f0a+CrNcyHmZm1sFpqaHsBrwFfAv5IOuy3ZzenOw2YlH9PAq4t7kHSCsA1wIUR8X9F3Ubmb5HOv93XzXh6ji9AMDOri0r3of0MuDQi/lLQ+oJy/XfSD4ArJB0OPAXsm6fZBhwVEUcA+wHbAcMlHZqHOzRf0XiJpBGke+JmAkf1UFxmZtakFGVqFJKOAQ4ARgKXA5c1++XxbW1t0d7eXu8w+g/JNVbrGil992X5KS6v9YihJ/Vg/JJmRERbt0fUyyrdWH1mRGwDbA8sBH4tabakkyRt2GcRmln/4/sHrQuqnkOLiCcj4tSI2Ar4FLAPMLvXIzMzM+uEWm6sHihpT0mXkG6ofhj4r16PzMzMrBMqXRSyC3Ag8DHgHmAqMDki/tNHsZmZmdWs0n1oJwKXAl+NiIV9FI+ZmVmXlE1oEbFjXwZiZmbWHTU/bd/MzKyROaGZmVlLqOUqx1NraWdmZlZPtdTQdinRbo+eDsTMrK58I3fTq3TZ/ueAzwPrS5pV0GkI3X8fmplZY3OCazqVLtu/lHQj9feB4wvav+zL+M3MGlw/TMiVLtv/N/Bv4EBJA4A1c/+DJQ2OiKf6KEYzM7Oqqr7gU9IU4GTgOaDjjdEBbNF7YZmZmXVOLW+s/iKwUUQs6O1gzMzMuqqWqxyfJh16NDMza1i11NAeA26VdB3wekfLiPhxr0VlZmbWSbUktKfyZ4X8MTMzazhVE1pEfBtA0so9+eoYSasBlwOjgSeA/SLihRL9vQX8Izc+FRETc/sxpFfarAb8FTgkIt7oqfjMzKy51PLoq20kPUB+S7WkLSX9vAemfTxwc0RsANzM0ve6FXotIsbmz8SC9qcCP8nDvwAc3gMxmZlZk6rlopAzgN2ABQAR8Xdgux6Y9l7ABfn3BcDetQ4oScBHgCu7MryZmbWemp62HxFPF7V6qwemvWZEPJvH/yywRpn+Bklql3SXpI6kNRx4MSIW5+Y5wNqlBpY0OQ/fPn/+/B4I28zMGlEtF4U8LemDQEhaATiafPixGkk3Ae8u0enrtYfIuhExV9L6wC2S/gG8VKK/ks95iYizgbMB2tra+t+zYMzM+olaEtpRwJmkGtAc4AbgC7WMPCJ2LtdN0nOSRkbEs5JGAvPKjGNu/n5M0q3AVsBVwFBJy+da2ihgbi0xmZlZa6p6yDEino+IgyJizYhYIyIO7qGnhkwDJuXfk4Bri3uQNEzSivn36sCHgAciIoDpwCcrDW9mZv1HpdfHHBsRP5T0P5Q4nBcRR3dz2j8ArpB0OOk+t33zdNuAoyLiCGAT4JeS3iYl3x9ExAN5+OOAqZJOAf4GnNvNeMzMrIlVOuTYcZ6svTcmnGt5O5Vo3w4ckX//Bdi8zPCPAeN7IzYzM2s+lV4f89v8fUG5fszMzBpFLTdW3yhpaEHzMEnX925YZmZmnVPLfWgjIuLFjob8eKpy94yZmZnVRS0J7S1J63Y0SFqPMvd8mZmZ1Ust96F9HbhD0m25eTtgcu+FZGZm1nm1PG3/j5K2BiYAAr4UEc/3emRmZmadUPaQo6SN8/fWwLqkJ3E8A6yb25mZmTWMSjW0L5MOLf6oRLcgPe3ezMysIVRKaDfm78PzTcxmZmYNq9JVjifk7ysr9GNmZtYQKtXQFkqaDqwvaVpxx6K3R5uZmdVVpYT2UWBr4CJKn0czMzNrGJUS2rkRcYikX0XEbRX6MzMzq7tK59Den58KclB+fuNqhZ++CtDMzKwWlWpo/wv8EVgfmEG6qbpD5PZmZmYNoWwNLSJ+GhGbAOdFxPoRMabg42RmZmYNperDiSPic5K2lfQZAEmrSxrT+6GZmZnVrpb3oX0LOI537ktbAbi4N4MyMzPrrFpeH7MPMBH4D0BEzAWG9GZQZmZmnVVLQnsjIoL8DjRJK3d3ovlKyRslPZK/h5XoZ0dJMws+iyTtnbudL+nxgm5juxuTmZk1t1oS2hWSfgkMlXQkcBPwq25O93jg5ojYALg5Ny8lIqZHxNiIGEt6EPKrwA0FvXyto3tEzOxmPGZm1uRqeR/a6ZJ2AV4CNgJOiogbqwxWzV7ADvn3BcCtpPN05XwS+ENEvNrN6ZqZWYuqpYYGMAu4jZR4/t4D010zIp4FyN9rVOn/AOCyonbfkzRL0k8krVhuQEmTJbVLap8/f373ojYzs4ZVy1WO+wH3APsC+wF3S/pkDcPdJOm+Ep+9OhOgpJHA5sD1Ba1PADYGxgGrUaF2FxFnR0RbRLSNGDGiM5M2M7MmUvWQI/B1YFxEzAOQNIJ0Hq3ia2UiYudy3SQ9J2lkRDybE9a8CqPaD7gmIt4sGPez+efrkn4NfLWG+TAzsxZWyyHH5TqSWbagxuEqmQZMyr8nAddW6PdAig435iSIJAF7A/d1Mx4zM2tytdTQ/ijpet5JKvsDf+jmdH9AunrycOAp0uFMJLUBR0XEEbl5NLAO6fxdoUtyTVHATOCobsZjZmZNTukWsyo9SZ8AtiUlkNsj4preDqw3tLW1RXt7e73D6D8kqKF8mVljkzQjItrqHUc1ZWtokt5LuhrxzxFxNXB1br+dpPdExD/7KkgzM7NqKp0LOwN4uUT7V3M3MzOzhlEpoY2OiFnFLSOiHRjdaxGZmZl1QaWENqhCt5V6OhAzM7PuqJTQ7s3PblxKvjJxRu+FZGZm1nmVLtv/InCNpIN4J4G1kd6Htk9vB2ZmZtYZZRNaRDwHfFDSjsBmufV1EXFLn0RmZmbWCbU8bX86ML0PYjEzM+uy7j7CyszMrCE4oZmZWUtwQjMzs5bghGZmZi3BCc3MzFqCE5qZmbUEJzQzM2sJTmhmZtYSnNDMzPb+bXEAABHJSURBVKwlOKGZmVlLqFtCk7SvpPslvS2p7Ku9Je0u6SFJj0o6vqD9GEl3S3pE0uWSVuibyM3MrBHVs4Z2H/AJ4PZyPUgaAJwF7AFsChwoadPc+VTgJxGxAfACcHjvhmtmZo2sbgktImZHxENVehsPPBoRj0XEG8BUYC9JAj4CXJn7uwDYu/eiNTOzRtfo59DWBp4uaJ6T2w0HXoyIxUXtlyFpsqR2Se3z58/v1WDNzKx+qr4+pjsk3QS8u0Snr0fEtbWMokS7qNB+2ZYRZwNnA7S1tZXsx8zMml+vJrSI2Lmbo5gDrFPQPAqYCzwPDJW0fK6ldbQ3M7N+qtEPOd4LbJCvaFwBOACYFhFBeunoJ3N/k4BaanxmZtai6nnZ/j6S5gDbANdJuj63X0vS7wFy7WsKcD0wG7giIu7PozgO+LKkR0nn1M7t63kwM7PGoVTZ6R/a2tqivb293mH0HxL0o/Jl1qokzYiIsvcLN4pGP+RoZmZWEyc0MzNrCU5oZmbWEpzQzMysJTihmZlZS3BCMzOzltCrTwoxs8bw5ptvMmfOHBYtWlTvUKyBDRo0iFGjRjFw4MB6h9IlTmhm/cCcOXMYMmQIo0ePJr2swmxpEcGCBQuYM2cOY8aMqXc4XeJDjmb9wKJFixg+fLiTmZUlieHDhzd1Ld4JzayfcDKzapq9jDihmZlZS3BCM+uPpJ791GDw4ME9PhsDBgxg7NixbLbZZuy55568+OKLPT4Nax5OaGbWtFZaaSVmzpzJfffdx2qrrcZZZ53VI+NdvHhxj4zH+pYTmpnVzZNPPslOO+3EFltswU477cRTTz0FwD//+U8mTJjAuHHjOOmkk2qq3W2zzTY888wzS5pPO+00xo0bxxZbbMG3vvWtJe2/+93vsvHGG7PLLrtw4IEHcvrppwOwww47cOKJJ7L99ttz5pln9vCcWl9wQjOzupkyZQqf/vSnmTVrFgcddBBHH300AMcccwzHHHMM9957L2uttVbV8bz11lvcfPPNTJw4EYAbbriBRx55hHvuuYeZM2cyY8YMbr/9dtrb27nqqqv429/+xtVXX03x66RefPFFbrvtNr7yla/0/Mxar3NCM7O6ufPOO/nUpz4FwCGHHMIdd9yxpP2+++4LsKR7Ka+99hpjx45l+PDhLFy4kF122QVICe2GG25gq622Yuutt+bBBx/kkUce4Y477mCvvfZipZVWYsiQIey5555LjW///ffvjdm0PuKEZmYNo7OXjXecQ3vyySd54403lpxDiwhOOOEEZs6cycyZM3n00Uc5/PDDqfZC45VXXrnLsVv9OaGZWd188IMfZOrUqQBccsklbLvttgBMmDCBq666CmBJ90pWXXVVfvrTn3L66afz5ptvsttuu3HeeefxyiuvAPDMM88wb948tt12W37729+yaNEiXnnlFa677rpemjOrBz/6yqw/qlJT6Q2vvvoqo0aNWtL85S9/mZ/+9KccdthhnHbaaYwYMYJf//rXAJxxxhkcfPDB/OhHP+JjH/sYq666atXxb7XVVmy55ZZMnTqVQw45hNmzZ7PNNtsA6ZaBiy++mHHjxjFx4kS23HJL1ltvPdra2moatzUHVauC98pEpX2Bk4FNgPER0V6in3WAC4F3A28DZ0fEmbnbycCRwPzc+4kR8ftq021ra4vik8DWi6S67DhtWbNnz2aTTTapdxg1e/XVV1lppZWQxNSpU7nsssu49tpre2Tcr7zyCoMHD+bVV19lu+224+yzz2brrbfukXG3glJlRdKMiGirU0g1q1cN7T7gE8AvK/SzGPhKRPxV0hBghqQbI+KB3P0nEXF6bwdqZn1vxowZTJkyhYhg6NChnHfeeT027smTJ/PAAw+waNEiJk2a5GTWQuqS0CJiNlQ+ARwRzwLP5t8vS5oNrA08UHYgM2sJH/7wh/n73//eK+O+9NJLe2W8Vn9NcVGIpNHAVsDdBa2nSJol6TxJwyoMO1lSu6T2+fPnl+vNzMyaXK8lNEk3SbqvxGevTo5nMHAV8MWIeCm3/gXwHmAsqRb3o3LDR8TZEdEWEW0jRozo4tyYmVmj67VDjhGxc3fHIWkgKZldEhFXF4z7uYJ+fgX8rrvTMjOz5tawhxyVTrCdC8yOiB8XdRtZ0LgP6SITMzPrx+pyUYikfYD/AUYA10maGRG7SVoLOCciPgp8CDgE+IekmXnQjsvzfyhpLBDAE8Bn+3wmzJqYvt2zL3KMb1W/PWPw4MFLbnTuKQMGDGDzzTdn8eLFjBkzhosuuoihQ4f26DS6YvTo0bS3t7P66qsvaXfJJZdw6qmnAmlZ/OIXv2DLLbcsOeyQIUOQxLBhw7jwwgtZb731ejS+J554go9//OPcd19r1QXqUkOLiGsiYlRErBgRa0bEbrn93JzMiIg7IkIRsUVEjM2f3+duh0TE5rnbxHxFpJn1M/V4fcyhhx7Krbfe2ulxjhkzhttuu41Zs2bxzW9+k8mTJ5ftd/r06cyaNYsddtiBU045pdPT6q8a9pCjmbW+/vT6mA9+8IMMG5YuyJ4wYQJz5sypOkzxPF188cWMHz+esWPH8tnPfpa33noLgHPPPZcNN9yQHXbYgSOPPJIpU6YAKfleeeWVS4bvjZesNhInNDOrm/76+phzzz2XPfbYo2p/f/zjH9l7772B9ASPyy+/nD//+c/MnDmTAQMGcMkllzB37ly++93vctddd3HjjTfy4IMP9mrsjczPcjSzurnzzju5+up0AfMhhxzCscceu6T9b37zGyC9PuarX/1qyeE7Xh/zxBNP8P73v7/k62MgPe7qkUce4eWXX17y+hig5tfHXH/99Rx33HEAPPXUU9xxxx0MHjyYFVdckbvvvrvkMOVMnz6dc889d8mrckrZcccdee6551hjjTWWHHK8+eabmTFjBuPGjVsy72ussQb33HMP22+/PautthoA++67Lw8//HCnYmoVrqGZWcNo1NfH7LbbbkvGNXHiRM455xxmzpzZ6WQ2a9YsjjjiCK699lqGDx9etr/p06fz5JNP8r73vY+TTjppyTxNmjRpSRwPPfQQJ598csV5Wn755Xn77beXDP/GG290Kt5m44RmZnXTn14f89RTT/GJT3yCiy66iA033LBq/yuttBJnnHEGF154IQsXLmSnnXbiyiuvZN68eQAsXLiQJ598kvHjx3PbbbfxwgsvsHjx4iXLDdIVkzNmzADg2muv5c033+ydmWsQPuRo1g/Vcpl9T+tvr4/ZYostWG65VGfYb7/9eOmll1iwYAGf//zngVR7qvb2j5EjR3LggQdy1lln8c1vfpNTTjmFXXfdlbfffpuBAwdy1llnMWHCBE488UQ+8IEPsNZaa7HpppsumacjjzySvfbai/Hjx7PTTju1/AtM6/L6mHrx62Osv/LrY97Riq+P6ZinxYsXs88++3DYYYexzz77dGlcfn2MmVkP8utjOufkk0/mpptuYtGiRey6665Lrozsb1xDM+sHmq2GZvXTzDU0XxRi1k/0pz+v1jXNXkac0Mz6gUGDBrFgwYKm32FZ74kIFixYwKBBg+odSpf5HJpZPzBq1CjmzJmDX3JrlQwaNGipK1GbjROaWT8wcOBAxowZU+8wzHqVDzmamVlLcEIzM7OW4IRmZmYtoV/dhyZpPvBkjb2vDjzfi+H0NsdfX80cfzPHDo6/N6wXESPqHUQ1/SqhdYak9ma4kbAcx19fzRx/M8cOjr8/8yFHMzNrCU5oZmbWEpzQyju73gF0k+Ovr2aOv5ljB8ffb/kcmpmZtQTX0MzMrCU4oZmZWUtwQitB0u6SHpL0qKTj6x1PNZLOkzRP0n0F7VaTdKOkR/L3sHrGWI6kdSRNlzRb0v2SjsntmyX+QZLukfT3HP+3c/sxku7O8V8uaYV6x1qJpAGS/ibpd7m5aeKX9ISkf0iaKak9t2uW8jNU0pWSHszbwDbNEnsjckIrImkAcBawB7ApcKCkTesbVVXnA7sXtTseuDkiNgBuzs2NaDHwlYjYBJgAfCEv72aJ/3XgIxGxJTAW2F3SBOBU4Cc5/heAw+sYYy2OAWYXNDdb/DtGxNiC+7eapfycCfwxIjYGtiStg2aJvfFEhD8FH2Ab4PqC5hOAE+odVw1xjwbuK2h+CBiZf48EHqp3jDXOx7XALs0YP/Au4K/AB0hPeli+VJlqtA8wirTj/AjwO0BNFv8TwOpF7Rq+/ACrAI+TL85rptgb9eMa2rLWBp4uaJ6T2zWbNSPiWYD8vUad46lK0mhgK+Bumij+fLhuJjAPuBH4J/BiRCzOvTR6GToDOBZ4OzcPp7niD+AGSTMkTc7tmqH8rA/MB36dD/eeI2llmiP2huSEtiyVaOd7G3qZpMHAVcAXI+KlesfTGRHxVkSMJdV0xgOblOqtb6OqjaSPA/MiYkZh6xK9NmT82YciYmvSaYIvSNqu3gHVaHlga+AXEbEV8B98eLFbnNCWNQdYp6B5FDC3TrF0x3OSRgLk73l1jqcsSQNJyeySiLg6t26a+DtExIvAraRzgUMldbxAt5HL0IeAiZKeAKaSDjueQfPET0TMzd/zgGtIfyqaofzMAeZExN25+UpSgmuG2BuSE9qy7gU2yFd5rQAcAEyrc0xdMQ2YlH9PIp2bajiSBJwLzI6IHxd0apb4R0gamn+vBOxMOrE/Hfhk7q1h44+IEyJiVESMJpX1WyLiIJokfkkrSxrS8RvYFbiPJig/EfEv4GlJG+VWOwEP0ASxNyo/KaQESR8l/UsdAJwXEd+rc0gVSboM2IH02onngG8BvwGuANYFngL2jYiF9YqxHEnbAn8C/sE753BOJJ1Ha4b4twAuIJWV5YArIuI7ktYn1XhWA/4GHBwRr9cv0uok7QB8NSI+3izx5zivyY3LA5dGxPckDac5ys9Y4BxgBeAx4DPkckSDx96InNDMzKwl+JCjmZm1BCc0MzNrCU5oZmbWEpzQzMysJTihmZlZS3BCs35L0rslTZX0T0kPSPq9pA27OK6j89PSL5G0oqSb8tPf98+PNCr7gGtJE7v6Vof8tPbPV+j+Spn250v6ZKluZs1q+eq9mLWefEP3NcAFEXFAbjcWWBN4uAuj/DywR0Q8np+2PzA/Dgvg8koDRsQ0un7z/tA87Z93cXizluEamvVXOwJvRsT/drSIiJkR8Sclp0m6L79na/+OfiR9TdK9kmYVvPvsf0kPmp0m6TjgYmBsrqG9R9Ktktpyv7tL+qvS+9Nuzu0OlfSz/HuEpKvyNO6V9KHc/mSl997dKukxSUfnkH4AvCdP67RyM5vn6We5Jnod+YG3klZVevffRrn5MklH9swiNutbrqFZf7UZMKNMt0+Q3m22JenpK/dKuh3YHNiA9KxAkRLYdhFxlKTdSe/kel7S3eQnbgCkymBKVsCvgO1yTW61EtM+k/QesjskrQtczzsPO96YlIiHAA9J+gXpYbabFdQGy9kH2CjPw5qkRyydFxH/ljQFOF/SmcCwiPhVlXGZNSQnNLNlbQtcFhFvkR4UexswDtiO9KzAv+X+BpMS3O01jncCcHtEPA5Q5nFGOwObdiRBYJWOZxUC1+XHT70uaR4pMdVqu4J5mivplo4OEXGjpH1JL7bdshPjNGsoTmjWX93POw/fLVbq9Skd7b8fEb/s4jRF9dewLAdsExGvLTVgSnCFz1J8i85vvyWnLWk5Ui3wNdKzG+d0crxmDcHn0Ky/ugVYsfB8kaRxkrYn1bj2V3px5whS7eYe0uG/w5Te3YaktSV15uWLdwLbSxqThy91yPEGYEpBTNUOJb5MOgRZze3AAXmeRpIOXXb4EukNAQcC5ym9zses6biGZv1SRISkfYAz8iXzi4AngC+Sdv7bAH8n1WqOza/6+JekTYA7c43pFeBganxfVUTMV3qj8tW5VjQP2KWot6OBsyTNIm2ftwNHVRjnAkl/lnQf8IeI+FqZXq8hvevsH6SrOG8DyLcpHAGMj4iX87nCb5De2GDWVPy0fTMzawk+5GhmZi3BCc3MzFqCE5qZmbUEJzQzM2sJTmhmZtYSnNDMzKwlOKGZmVlL+P+PbkGCvq4NNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the coefficients from logistic regression\n",
    "\n",
    "print(\"Coefficients obtained using the entire training set without regularization: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "#print the coefficients from L2 regularized logistic regression\n",
    "\n",
    "print(\"Coefficients obtained using the entire training set with regularization: {}\".format( regL2_best.coef_ ))\n",
    "\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( regL2_best.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "regL2_best_coef = regL2_best.coef_.reshape(regL2_best.coef_.shape[1],)\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(ind, reg_coef, width, color='r')\n",
    "rects2 = ax.bar(ind + width, regL2_best_coef, width, color='g')\n",
    "ax.legend((rects1[0], rects2[0]), ('Log Regr', 'Log Regr + L2 Regul'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients: Standard and Regularized Version')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7475\n",
      "Test Accuracy: 0.6626506024096386\n",
      "Accuracy for training dataset: 0.775\n",
      "Accuracy for test dataset: 0.6345381526104418\n"
     ]
    }
   ],
   "source": [
    "#now get training and test error and print training and test accuracy\n",
    "\n",
    "# predictions on training data \n",
    "Y_training_prediction_LR_L2 = regL2_best.predict(X_training)\n",
    "\n",
    "# predictions on test data \n",
    "Y_test_prediction_LR_L2 = regL2_best.predict(X_test)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on training data\n",
    "print(\"Training Accuracy:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR_L2))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on test data\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR_L2))\n",
    "\n",
    "\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "\n",
    "print(\"Accuracy for training dataset:\", metrics.accuracy_score(Y_training, Y_training_prediction_LR))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "\n",
    "print(\"Accuracy for test dataset:\", metrics.accuracy_score(Y_test, Y_test_prediction_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TO DO 10: Discuss all the questions above for the larger set (max 7 lines). Compare the impact of regularization in this case with the results you obtained for the smaller set.\n",
    "\n",
    "### Answer to the question\n",
    "With a larger set the accuracy on training with L2 regularization is still lower than the one without regularization, but the test accuracy is higher with regularization. Beside that, in this case both logistic regression with and without cross validation show values really close to zero. Even in this case the logistic regression with L2 regularization shows values in general closer to zero than the logistic regression without regularization."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
